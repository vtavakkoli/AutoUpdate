{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACGAN_CIFAR_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Syt6Ln6rxljHjS_ug12TqlpSFpAq4zmp",
      "authorship_tag": "ABX9TyMF3hsT2UiQrmTRLAHc7lSX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtavakkoli/AutoUpdate/blob/master/GAN/ACGAN_CIFAR_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D,LeakyReLU, UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "T6C6_-X4Hvlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Gl9u_h0OVa"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class ACGAN():\n",
        "    def __init__(self):\n",
        "        # Input shape\n",
        "        self.img_rows = 32\n",
        "        self.img_cols = 32\n",
        "        self.channels = 3\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 10\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = Adam(0.0002, 0.5)\n",
        "        losses = ['binary_crossentropy', 'sparse_categorical_crossentropy']\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss=losses,\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise and the target label as input\n",
        "        # and generates the corresponding digit of that label\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,))\n",
        "        img = self.generator([noise, label])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated image as input and determines validity\n",
        "        # and the label of that image\n",
        "        valid, target_label = self.discriminator(img)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model([noise, label], [valid, target_label])\n",
        "        self.combined.compile(loss=losses,\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
        "        model.add(Reshape((7, 7, 128)))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(UpSampling2D())\n",
        "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n",
        "        model.add(Activation(\"tanh\"))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "        model_input = multiply([noise, label_embedding])\n",
        "        img = model(model_input)\n",
        "\n",
        "        return Model([noise, label], img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.summary()\n",
        "\n",
        "        img = Input(shape=self.img_shape)\n",
        "\n",
        "        # Extract feature representation\n",
        "        features = model(img)\n",
        "\n",
        "        # Determine validity and label of the image\n",
        "        validity = Dense(1, activation=\"sigmoid\")(features)\n",
        "        label = Dense(self.num_classes, activation=\"softmax\")(features)\n",
        "\n",
        "        return Model(img, [validity, label])\n",
        "\n",
        "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        (X_train, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Configure inputs\n",
        "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Sample noise as generator input\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # The labels of the digits that the generator tries to create an\n",
        "            # image representation of\n",
        "            sampled_labels = np.random.randint(0, 10, (batch_size, 1))\n",
        "\n",
        "            # Generate a half batch of new images\n",
        "            gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "            # Image labels. 0-9 \n",
        "            img_labels = y_train[idx]\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs, [valid, img_labels])\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, [fake, sampled_labels])\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.combined.train_on_batch([noise, sampled_labels], [valid, sampled_labels])\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss[0]))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % sample_interval == 0:\n",
        "                self.save_model()\n",
        "                self.sample_images(epoch)\n",
        "\n",
        "    def sample_images(self, epoch):\n",
        "        r, c = 10, 10\n",
        "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
        "        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
        "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"%d.png\" % epoch)\n",
        "        plt.close()\n",
        "\n",
        "    def save_model(self):\n",
        "\n",
        "        def save(model, model_name):\n",
        "            model_path = \"%s.json\" % model_name\n",
        "            weights_path = \"%s_weights.hdf5\" % model_name\n",
        "            options = {\"file_arch\": model_path,\n",
        "                        \"file_weight\": weights_path}\n",
        "            json_string = model.to_json()\n",
        "            open(options['file_arch'], 'w').write(json_string)\n",
        "            model.save_weights(options['file_weight'])\n",
        "\n",
        "        save(self.generator, \"generator\")\n",
        "        save(self.discriminator, \"discriminator\")\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acgan = ACGAN()\n"
      ],
      "metadata": {
        "id": "0pd_UKshHngU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4275b603-cae2-4d60-d4a9-78000c5a7e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 14, 14, 16)        160       \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 14, 14, 16)        0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 16)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 7, 7, 32)          4640      \n",
            "                                                                 \n",
            " zero_padding2d (ZeroPadding  (None, 8, 8, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 32)          0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8, 8, 32)         128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 4, 64)          18496     \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 64)          0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 64)          0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 4, 4, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 97,536\n",
            "Trainable params: 97,344\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 6272)              633472    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 7, 7, 128)         0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 7, 7, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2D  (None, 14, 14, 128)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 128)       147584    \n",
            "                                                                 \n",
            " activation (Activation)     (None, 14, 14, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 14, 14, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSampling  (None, 28, 28, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 28, 28, 64)        73792     \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 28, 28, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 28, 28, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 1)         577       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 856,705\n",
            "Trainable params: 856,065\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acgan.train(epochs=1750, batch_size=256, sample_interval=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9zlHSrRIpf7",
        "outputId": "52d2e6b4-394d-4d3b-8354-49e62317101f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "0 [D loss: 3.198477, acc.: 39.26%, op_acc: 10.16%] [G loss: 2.976813]\n",
            "1 [D loss: 3.277834, acc.: 54.10%, op_acc: 12.70%] [G loss: 2.948420]\n",
            "2 [D loss: 3.051213, acc.: 71.48%, op_acc: 10.74%] [G loss: 2.922574]\n",
            "3 [D loss: 2.829663, acc.: 87.70%, op_acc: 13.48%] [G loss: 2.893007]\n",
            "4 [D loss: 2.698270, acc.: 96.88%, op_acc: 11.91%] [G loss: 2.828995]\n",
            "5 [D loss: 2.658080, acc.: 97.07%, op_acc: 14.65%] [G loss: 2.708800]\n",
            "6 [D loss: 2.608462, acc.: 99.02%, op_acc: 11.33%] [G loss: 2.707874]\n",
            "7 [D loss: 2.529116, acc.: 100.00%, op_acc: 14.45%] [G loss: 2.582389]\n",
            "8 [D loss: 2.523062, acc.: 99.22%, op_acc: 14.84%] [G loss: 2.591905]\n",
            "9 [D loss: 2.454090, acc.: 100.00%, op_acc: 16.02%] [G loss: 2.580392]\n",
            "10 [D loss: 2.555335, acc.: 91.99%, op_acc: 17.58%] [G loss: 2.756752]\n",
            "11 [D loss: 2.585266, acc.: 93.95%, op_acc: 16.99%] [G loss: 2.820682]\n",
            "12 [D loss: 2.505318, acc.: 89.26%, op_acc: 18.95%] [G loss: 2.945037]\n",
            "13 [D loss: 2.504287, acc.: 93.36%, op_acc: 17.58%] [G loss: 3.020769]\n",
            "14 [D loss: 2.378740, acc.: 99.41%, op_acc: 19.14%] [G loss: 2.928862]\n",
            "15 [D loss: 2.358874, acc.: 99.61%, op_acc: 22.46%] [G loss: 2.968937]\n",
            "16 [D loss: 2.399667, acc.: 93.95%, op_acc: 23.63%] [G loss: 3.085582]\n",
            "17 [D loss: 2.514161, acc.: 85.35%, op_acc: 23.83%] [G loss: 3.045116]\n",
            "18 [D loss: 2.622624, acc.: 75.39%, op_acc: 22.85%] [G loss: 3.133039]\n",
            "19 [D loss: 2.604409, acc.: 75.39%, op_acc: 23.24%] [G loss: 3.081881]\n",
            "20 [D loss: 2.685528, acc.: 76.37%, op_acc: 22.07%] [G loss: 3.166695]\n",
            "21 [D loss: 2.655276, acc.: 76.95%, op_acc: 22.46%] [G loss: 3.043681]\n",
            "22 [D loss: 2.647573, acc.: 73.44%, op_acc: 27.15%] [G loss: 2.995039]\n",
            "23 [D loss: 2.739149, acc.: 61.13%, op_acc: 26.95%] [G loss: 2.984717]\n",
            "24 [D loss: 2.776683, acc.: 57.03%, op_acc: 29.69%] [G loss: 3.038298]\n",
            "25 [D loss: 2.812170, acc.: 50.59%, op_acc: 30.08%] [G loss: 3.210462]\n",
            "26 [D loss: 2.885693, acc.: 46.29%, op_acc: 28.91%] [G loss: 3.205391]\n",
            "27 [D loss: 2.985027, acc.: 39.84%, op_acc: 27.93%] [G loss: 3.416926]\n",
            "28 [D loss: 2.869753, acc.: 40.82%, op_acc: 31.64%] [G loss: 3.335969]\n",
            "29 [D loss: 2.948009, acc.: 31.45%, op_acc: 30.86%] [G loss: 3.325680]\n",
            "30 [D loss: 2.841711, acc.: 38.48%, op_acc: 32.62%] [G loss: 3.361876]\n",
            "31 [D loss: 2.781162, acc.: 46.29%, op_acc: 31.64%] [G loss: 3.424908]\n",
            "32 [D loss: 2.668504, acc.: 50.98%, op_acc: 33.79%] [G loss: 3.404311]\n",
            "33 [D loss: 2.659361, acc.: 49.80%, op_acc: 33.79%] [G loss: 3.216175]\n",
            "34 [D loss: 2.608581, acc.: 56.45%, op_acc: 34.96%] [G loss: 3.290360]\n",
            "35 [D loss: 2.584871, acc.: 57.42%, op_acc: 34.38%] [G loss: 3.299614]\n",
            "36 [D loss: 2.544706, acc.: 63.67%, op_acc: 32.81%] [G loss: 3.327047]\n",
            "37 [D loss: 2.460805, acc.: 62.70%, op_acc: 37.50%] [G loss: 3.339475]\n",
            "38 [D loss: 2.320644, acc.: 70.70%, op_acc: 40.23%] [G loss: 3.433481]\n",
            "39 [D loss: 2.366992, acc.: 70.70%, op_acc: 37.89%] [G loss: 3.377218]\n",
            "40 [D loss: 2.386834, acc.: 70.90%, op_acc: 35.35%] [G loss: 3.505168]\n",
            "41 [D loss: 2.302664, acc.: 75.78%, op_acc: 35.35%] [G loss: 3.386758]\n",
            "42 [D loss: 2.278181, acc.: 71.09%, op_acc: 39.84%] [G loss: 3.504135]\n",
            "43 [D loss: 2.322609, acc.: 74.22%, op_acc: 35.94%] [G loss: 3.386192]\n",
            "44 [D loss: 2.156711, acc.: 78.32%, op_acc: 41.99%] [G loss: 3.314592]\n",
            "45 [D loss: 2.198479, acc.: 75.00%, op_acc: 42.38%] [G loss: 3.262277]\n",
            "46 [D loss: 2.172080, acc.: 75.98%, op_acc: 38.87%] [G loss: 3.209444]\n",
            "47 [D loss: 2.166458, acc.: 74.41%, op_acc: 40.82%] [G loss: 2.999306]\n",
            "48 [D loss: 2.240937, acc.: 73.63%, op_acc: 38.28%] [G loss: 2.939882]\n",
            "49 [D loss: 2.197775, acc.: 68.95%, op_acc: 41.02%] [G loss: 3.004440]\n",
            "50 [D loss: 2.179740, acc.: 67.58%, op_acc: 42.97%] [G loss: 3.051929]\n",
            "51 [D loss: 2.137651, acc.: 71.29%, op_acc: 42.38%] [G loss: 3.028751]\n",
            "52 [D loss: 2.123669, acc.: 75.39%, op_acc: 41.99%] [G loss: 2.990603]\n",
            "53 [D loss: 2.140855, acc.: 73.24%, op_acc: 41.60%] [G loss: 2.919013]\n",
            "54 [D loss: 2.153942, acc.: 72.46%, op_acc: 41.99%] [G loss: 2.912123]\n",
            "55 [D loss: 2.125544, acc.: 77.54%, op_acc: 39.84%] [G loss: 2.969248]\n",
            "56 [D loss: 2.054771, acc.: 77.54%, op_acc: 47.66%] [G loss: 2.917202]\n",
            "57 [D loss: 1.937514, acc.: 76.95%, op_acc: 51.37%] [G loss: 2.723784]\n",
            "58 [D loss: 2.092047, acc.: 72.66%, op_acc: 45.31%] [G loss: 2.885121]\n",
            "59 [D loss: 2.051237, acc.: 76.17%, op_acc: 45.12%] [G loss: 2.786487]\n",
            "60 [D loss: 2.065389, acc.: 75.59%, op_acc: 43.95%] [G loss: 2.841056]\n",
            "61 [D loss: 1.985857, acc.: 77.73%, op_acc: 49.02%] [G loss: 2.811661]\n",
            "62 [D loss: 2.032113, acc.: 71.88%, op_acc: 45.12%] [G loss: 2.693921]\n",
            "63 [D loss: 2.041952, acc.: 67.38%, op_acc: 50.39%] [G loss: 2.720001]\n",
            "64 [D loss: 2.013794, acc.: 73.05%, op_acc: 46.29%] [G loss: 2.750827]\n",
            "65 [D loss: 2.033606, acc.: 70.70%, op_acc: 45.90%] [G loss: 2.676713]\n",
            "66 [D loss: 2.049326, acc.: 64.06%, op_acc: 46.48%] [G loss: 2.669723]\n",
            "67 [D loss: 2.019752, acc.: 67.58%, op_acc: 47.66%] [G loss: 2.729837]\n",
            "68 [D loss: 2.060718, acc.: 65.82%, op_acc: 44.34%] [G loss: 2.666953]\n",
            "69 [D loss: 2.022662, acc.: 59.38%, op_acc: 51.76%] [G loss: 2.674289]\n",
            "70 [D loss: 2.015880, acc.: 59.57%, op_acc: 50.78%] [G loss: 2.693503]\n",
            "71 [D loss: 1.997133, acc.: 58.20%, op_acc: 52.54%] [G loss: 2.701832]\n",
            "72 [D loss: 2.055364, acc.: 59.38%, op_acc: 48.83%] [G loss: 2.616554]\n",
            "73 [D loss: 2.072520, acc.: 56.05%, op_acc: 49.61%] [G loss: 2.626474]\n",
            "74 [D loss: 1.998669, acc.: 62.11%, op_acc: 53.91%] [G loss: 2.540239]\n",
            "75 [D loss: 2.020778, acc.: 58.79%, op_acc: 48.63%] [G loss: 2.679594]\n",
            "76 [D loss: 1.949895, acc.: 65.43%, op_acc: 53.32%] [G loss: 2.519053]\n",
            "77 [D loss: 1.902061, acc.: 70.90%, op_acc: 49.61%] [G loss: 2.662870]\n",
            "78 [D loss: 1.900044, acc.: 72.85%, op_acc: 49.41%] [G loss: 2.642808]\n",
            "79 [D loss: 1.825661, acc.: 76.37%, op_acc: 49.02%] [G loss: 2.543894]\n",
            "80 [D loss: 1.860099, acc.: 70.12%, op_acc: 54.49%] [G loss: 2.481731]\n",
            "81 [D loss: 1.836327, acc.: 72.66%, op_acc: 56.25%] [G loss: 2.477229]\n",
            "82 [D loss: 1.818395, acc.: 67.97%, op_acc: 56.45%] [G loss: 2.465569]\n",
            "83 [D loss: 1.984211, acc.: 59.57%, op_acc: 53.52%] [G loss: 2.472284]\n",
            "84 [D loss: 1.907684, acc.: 58.59%, op_acc: 54.69%] [G loss: 2.437639]\n",
            "85 [D loss: 1.801684, acc.: 62.89%, op_acc: 58.79%] [G loss: 2.413504]\n",
            "86 [D loss: 1.875199, acc.: 56.25%, op_acc: 56.64%] [G loss: 2.367942]\n",
            "87 [D loss: 1.931372, acc.: 57.03%, op_acc: 54.88%] [G loss: 2.398502]\n",
            "88 [D loss: 1.843107, acc.: 59.77%, op_acc: 56.25%] [G loss: 2.412525]\n",
            "89 [D loss: 1.928770, acc.: 60.94%, op_acc: 56.05%] [G loss: 2.436436]\n",
            "90 [D loss: 1.816082, acc.: 60.35%, op_acc: 58.79%] [G loss: 2.337744]\n",
            "91 [D loss: 1.852277, acc.: 60.94%, op_acc: 58.20%] [G loss: 2.323887]\n",
            "92 [D loss: 1.845554, acc.: 55.08%, op_acc: 60.55%] [G loss: 2.315434]\n",
            "93 [D loss: 1.849763, acc.: 57.03%, op_acc: 59.38%] [G loss: 2.248008]\n",
            "94 [D loss: 1.925559, acc.: 53.52%, op_acc: 54.49%] [G loss: 2.362677]\n",
            "95 [D loss: 1.754580, acc.: 62.50%, op_acc: 59.77%] [G loss: 2.392302]\n",
            "96 [D loss: 1.880986, acc.: 57.42%, op_acc: 55.86%] [G loss: 2.284283]\n",
            "97 [D loss: 1.698409, acc.: 58.20%, op_acc: 62.50%] [G loss: 2.232251]\n",
            "98 [D loss: 1.667771, acc.: 61.52%, op_acc: 64.06%] [G loss: 2.236975]\n",
            "99 [D loss: 1.704250, acc.: 59.77%, op_acc: 62.70%] [G loss: 2.270840]\n",
            "100 [D loss: 1.729596, acc.: 63.87%, op_acc: 60.35%] [G loss: 2.205654]\n",
            "101 [D loss: 1.728363, acc.: 60.74%, op_acc: 60.16%] [G loss: 2.093251]\n",
            "102 [D loss: 1.582716, acc.: 66.21%, op_acc: 64.06%] [G loss: 2.110286]\n",
            "103 [D loss: 1.675319, acc.: 60.94%, op_acc: 62.30%] [G loss: 2.063993]\n",
            "104 [D loss: 1.683326, acc.: 57.23%, op_acc: 62.50%] [G loss: 2.148098]\n",
            "105 [D loss: 1.655277, acc.: 65.04%, op_acc: 64.26%] [G loss: 2.144816]\n",
            "106 [D loss: 1.552018, acc.: 64.84%, op_acc: 66.99%] [G loss: 2.190133]\n",
            "107 [D loss: 1.688275, acc.: 60.55%, op_acc: 62.89%] [G loss: 2.170377]\n",
            "108 [D loss: 1.474807, acc.: 68.75%, op_acc: 67.38%] [G loss: 2.151774]\n",
            "109 [D loss: 1.451016, acc.: 74.80%, op_acc: 71.29%] [G loss: 2.156356]\n",
            "110 [D loss: 1.417813, acc.: 72.85%, op_acc: 67.38%] [G loss: 2.006035]\n",
            "111 [D loss: 1.446050, acc.: 70.12%, op_acc: 69.73%] [G loss: 1.917735]\n",
            "112 [D loss: 1.422626, acc.: 67.58%, op_acc: 71.88%] [G loss: 1.909796]\n",
            "113 [D loss: 1.523098, acc.: 66.99%, op_acc: 68.16%] [G loss: 1.873497]\n",
            "114 [D loss: 1.612785, acc.: 62.89%, op_acc: 66.80%] [G loss: 1.881860]\n",
            "115 [D loss: 1.536338, acc.: 66.21%, op_acc: 65.23%] [G loss: 1.933606]\n",
            "116 [D loss: 1.398585, acc.: 72.85%, op_acc: 70.31%] [G loss: 1.876370]\n",
            "117 [D loss: 1.437124, acc.: 66.99%, op_acc: 73.24%] [G loss: 1.861954]\n",
            "118 [D loss: 1.343965, acc.: 70.90%, op_acc: 75.59%] [G loss: 1.858158]\n",
            "119 [D loss: 1.449559, acc.: 71.48%, op_acc: 69.14%] [G loss: 1.972888]\n",
            "120 [D loss: 1.444522, acc.: 66.60%, op_acc: 71.88%] [G loss: 1.852136]\n",
            "121 [D loss: 1.419748, acc.: 68.36%, op_acc: 70.70%] [G loss: 1.842400]\n",
            "122 [D loss: 1.399356, acc.: 65.04%, op_acc: 71.68%] [G loss: 1.740229]\n",
            "123 [D loss: 1.494519, acc.: 56.64%, op_acc: 74.22%] [G loss: 1.718518]\n",
            "124 [D loss: 1.603549, acc.: 52.34%, op_acc: 68.36%] [G loss: 1.683611]\n",
            "125 [D loss: 1.593632, acc.: 50.00%, op_acc: 71.09%] [G loss: 1.814397]\n",
            "126 [D loss: 1.468120, acc.: 56.64%, op_acc: 73.44%] [G loss: 1.738013]\n",
            "127 [D loss: 1.466158, acc.: 55.86%, op_acc: 75.00%] [G loss: 1.828138]\n",
            "128 [D loss: 1.469385, acc.: 49.80%, op_acc: 73.83%] [G loss: 1.717516]\n",
            "129 [D loss: 1.515980, acc.: 48.83%, op_acc: 73.44%] [G loss: 1.681728]\n",
            "130 [D loss: 1.362428, acc.: 56.05%, op_acc: 79.10%] [G loss: 1.577893]\n",
            "131 [D loss: 1.347347, acc.: 62.70%, op_acc: 72.85%] [G loss: 1.598227]\n",
            "132 [D loss: 1.341054, acc.: 59.38%, op_acc: 77.54%] [G loss: 1.564497]\n",
            "133 [D loss: 1.333721, acc.: 61.72%, op_acc: 75.78%] [G loss: 1.593029]\n",
            "134 [D loss: 1.385074, acc.: 52.73%, op_acc: 77.73%] [G loss: 1.443365]\n",
            "135 [D loss: 1.332497, acc.: 63.09%, op_acc: 75.98%] [G loss: 1.511577]\n",
            "136 [D loss: 1.382251, acc.: 57.81%, op_acc: 76.76%] [G loss: 1.440180]\n",
            "137 [D loss: 1.369337, acc.: 60.74%, op_acc: 77.93%] [G loss: 1.443965]\n",
            "138 [D loss: 1.387121, acc.: 58.20%, op_acc: 76.56%] [G loss: 1.508185]\n",
            "139 [D loss: 1.260418, acc.: 65.43%, op_acc: 78.12%] [G loss: 1.651825]\n",
            "140 [D loss: 1.426239, acc.: 62.89%, op_acc: 73.83%] [G loss: 1.715213]\n",
            "141 [D loss: 1.429716, acc.: 59.96%, op_acc: 73.24%] [G loss: 1.843961]\n",
            "142 [D loss: 1.398900, acc.: 52.54%, op_acc: 79.10%] [G loss: 1.528806]\n",
            "143 [D loss: 1.331275, acc.: 54.88%, op_acc: 82.03%] [G loss: 1.507972]\n",
            "144 [D loss: 1.367101, acc.: 52.15%, op_acc: 80.66%] [G loss: 1.527744]\n",
            "145 [D loss: 1.438948, acc.: 53.12%, op_acc: 76.76%] [G loss: 1.462537]\n",
            "146 [D loss: 1.359195, acc.: 54.10%, op_acc: 80.27%] [G loss: 1.380850]\n",
            "147 [D loss: 1.351447, acc.: 51.56%, op_acc: 77.73%] [G loss: 1.368262]\n",
            "148 [D loss: 1.282165, acc.: 58.40%, op_acc: 81.64%] [G loss: 1.414347]\n",
            "149 [D loss: 1.373962, acc.: 54.69%, op_acc: 78.71%] [G loss: 1.421739]\n",
            "150 [D loss: 1.314617, acc.: 48.44%, op_acc: 84.57%] [G loss: 1.401358]\n",
            "151 [D loss: 1.322118, acc.: 49.02%, op_acc: 82.42%] [G loss: 1.430514]\n",
            "152 [D loss: 1.319425, acc.: 52.15%, op_acc: 84.18%] [G loss: 1.459560]\n",
            "153 [D loss: 1.305384, acc.: 54.88%, op_acc: 81.84%] [G loss: 1.520797]\n",
            "154 [D loss: 1.380745, acc.: 48.83%, op_acc: 80.08%] [G loss: 1.491905]\n",
            "155 [D loss: 1.323265, acc.: 46.68%, op_acc: 84.77%] [G loss: 1.372366]\n",
            "156 [D loss: 1.328275, acc.: 48.63%, op_acc: 81.25%] [G loss: 1.283811]\n",
            "157 [D loss: 1.261467, acc.: 54.10%, op_acc: 84.77%] [G loss: 1.204832]\n",
            "158 [D loss: 1.198376, acc.: 61.72%, op_acc: 83.59%] [G loss: 1.281863]\n",
            "159 [D loss: 1.297188, acc.: 51.17%, op_acc: 81.64%] [G loss: 1.238811]\n",
            "160 [D loss: 1.251149, acc.: 51.17%, op_acc: 83.59%] [G loss: 1.241537]\n",
            "161 [D loss: 1.202328, acc.: 55.47%, op_acc: 85.55%] [G loss: 1.246158]\n",
            "162 [D loss: 1.183970, acc.: 53.12%, op_acc: 84.96%] [G loss: 1.241014]\n",
            "163 [D loss: 1.221562, acc.: 57.42%, op_acc: 86.72%] [G loss: 1.242486]\n",
            "164 [D loss: 1.204969, acc.: 61.91%, op_acc: 83.40%] [G loss: 1.257237]\n",
            "165 [D loss: 1.130679, acc.: 64.45%, op_acc: 83.59%] [G loss: 1.264384]\n",
            "166 [D loss: 1.098241, acc.: 72.46%, op_acc: 83.98%] [G loss: 1.287863]\n",
            "167 [D loss: 1.101966, acc.: 71.29%, op_acc: 83.98%] [G loss: 1.302718]\n",
            "168 [D loss: 1.079369, acc.: 69.73%, op_acc: 85.55%] [G loss: 1.316396]\n",
            "169 [D loss: 1.050280, acc.: 70.31%, op_acc: 87.30%] [G loss: 1.249492]\n",
            "170 [D loss: 1.111341, acc.: 64.06%, op_acc: 85.16%] [G loss: 1.342361]\n",
            "171 [D loss: 1.170981, acc.: 57.81%, op_acc: 85.16%] [G loss: 1.221253]\n",
            "172 [D loss: 1.157607, acc.: 56.45%, op_acc: 87.11%] [G loss: 1.226280]\n",
            "173 [D loss: 1.162545, acc.: 54.88%, op_acc: 86.33%] [G loss: 1.214675]\n",
            "174 [D loss: 1.301093, acc.: 48.44%, op_acc: 82.62%] [G loss: 1.297546]\n",
            "175 [D loss: 1.244903, acc.: 50.20%, op_acc: 83.98%] [G loss: 1.365322]\n",
            "176 [D loss: 1.197789, acc.: 47.27%, op_acc: 85.74%] [G loss: 1.146730]\n",
            "177 [D loss: 1.293378, acc.: 45.31%, op_acc: 87.11%] [G loss: 1.096043]\n",
            "178 [D loss: 1.312616, acc.: 42.77%, op_acc: 86.13%] [G loss: 1.182631]\n",
            "179 [D loss: 1.248317, acc.: 51.76%, op_acc: 84.18%] [G loss: 1.173183]\n",
            "180 [D loss: 1.211698, acc.: 44.73%, op_acc: 84.38%] [G loss: 1.258839]\n",
            "181 [D loss: 1.139765, acc.: 56.84%, op_acc: 88.87%] [G loss: 1.236871]\n",
            "182 [D loss: 1.136274, acc.: 56.25%, op_acc: 86.72%] [G loss: 1.220167]\n",
            "183 [D loss: 1.169642, acc.: 55.47%, op_acc: 85.94%] [G loss: 1.214880]\n",
            "184 [D loss: 1.104877, acc.: 53.52%, op_acc: 88.87%] [G loss: 1.164689]\n",
            "185 [D loss: 1.170816, acc.: 52.15%, op_acc: 86.52%] [G loss: 1.110284]\n",
            "186 [D loss: 1.114199, acc.: 55.86%, op_acc: 86.52%] [G loss: 1.164035]\n",
            "187 [D loss: 1.206119, acc.: 55.27%, op_acc: 83.59%] [G loss: 1.150195]\n",
            "188 [D loss: 1.259919, acc.: 45.12%, op_acc: 85.74%] [G loss: 1.085561]\n",
            "189 [D loss: 1.240236, acc.: 50.78%, op_acc: 83.79%] [G loss: 1.167524]\n",
            "190 [D loss: 1.158840, acc.: 49.02%, op_acc: 89.26%] [G loss: 1.221416]\n",
            "191 [D loss: 1.191635, acc.: 53.52%, op_acc: 87.11%] [G loss: 1.175269]\n",
            "192 [D loss: 1.194070, acc.: 55.47%, op_acc: 84.18%] [G loss: 1.207134]\n",
            "193 [D loss: 1.141575, acc.: 54.10%, op_acc: 85.74%] [G loss: 1.254628]\n",
            "194 [D loss: 1.110794, acc.: 61.91%, op_acc: 86.91%] [G loss: 1.180840]\n",
            "195 [D loss: 1.109032, acc.: 59.57%, op_acc: 86.52%] [G loss: 1.223994]\n",
            "196 [D loss: 1.043604, acc.: 57.23%, op_acc: 89.84%] [G loss: 1.193530]\n",
            "197 [D loss: 1.105968, acc.: 62.50%, op_acc: 85.16%] [G loss: 1.153701]\n",
            "198 [D loss: 1.067767, acc.: 55.08%, op_acc: 88.28%] [G loss: 1.107307]\n",
            "199 [D loss: 1.068769, acc.: 58.40%, op_acc: 88.87%] [G loss: 1.014558]\n",
            "200 [D loss: 1.078966, acc.: 59.38%, op_acc: 88.87%] [G loss: 1.058696]\n",
            "201 [D loss: 1.068029, acc.: 57.62%, op_acc: 89.84%] [G loss: 1.114085]\n",
            "202 [D loss: 1.080243, acc.: 57.62%, op_acc: 88.67%] [G loss: 1.117276]\n",
            "203 [D loss: 1.118931, acc.: 59.18%, op_acc: 85.55%] [G loss: 1.084726]\n",
            "204 [D loss: 1.074578, acc.: 60.35%, op_acc: 87.50%] [G loss: 1.135299]\n",
            "205 [D loss: 1.115726, acc.: 53.32%, op_acc: 87.50%] [G loss: 1.170825]\n",
            "206 [D loss: 1.044635, acc.: 58.20%, op_acc: 90.62%] [G loss: 1.119914]\n",
            "207 [D loss: 1.085999, acc.: 50.39%, op_acc: 88.87%] [G loss: 1.113925]\n",
            "208 [D loss: 1.098129, acc.: 54.88%, op_acc: 89.06%] [G loss: 1.112724]\n",
            "209 [D loss: 1.238934, acc.: 47.07%, op_acc: 88.09%] [G loss: 1.088407]\n",
            "210 [D loss: 1.038183, acc.: 60.74%, op_acc: 88.87%] [G loss: 1.067103]\n",
            "211 [D loss: 1.104619, acc.: 46.88%, op_acc: 90.23%] [G loss: 1.036437]\n",
            "212 [D loss: 1.164162, acc.: 53.52%, op_acc: 86.52%] [G loss: 1.090766]\n",
            "213 [D loss: 1.120020, acc.: 53.32%, op_acc: 88.09%] [G loss: 1.127527]\n",
            "214 [D loss: 1.107154, acc.: 53.52%, op_acc: 87.89%] [G loss: 1.158975]\n",
            "215 [D loss: 1.138288, acc.: 49.61%, op_acc: 89.06%] [G loss: 1.159073]\n",
            "216 [D loss: 1.171234, acc.: 45.12%, op_acc: 90.23%] [G loss: 1.032363]\n",
            "217 [D loss: 1.136499, acc.: 49.80%, op_acc: 87.30%] [G loss: 1.137898]\n",
            "218 [D loss: 1.141506, acc.: 53.32%, op_acc: 88.09%] [G loss: 1.109243]\n",
            "219 [D loss: 1.089950, acc.: 57.62%, op_acc: 89.45%] [G loss: 1.077887]\n",
            "220 [D loss: 1.080765, acc.: 63.48%, op_acc: 88.09%] [G loss: 1.107732]\n",
            "221 [D loss: 1.003747, acc.: 63.48%, op_acc: 87.70%] [G loss: 1.110880]\n",
            "222 [D loss: 1.046711, acc.: 58.79%, op_acc: 89.84%] [G loss: 1.119744]\n",
            "223 [D loss: 1.035744, acc.: 62.89%, op_acc: 88.87%] [G loss: 1.153705]\n",
            "224 [D loss: 1.050137, acc.: 60.94%, op_acc: 87.11%] [G loss: 1.140549]\n",
            "225 [D loss: 1.094546, acc.: 49.61%, op_acc: 89.26%] [G loss: 1.043901]\n",
            "226 [D loss: 1.057207, acc.: 54.30%, op_acc: 90.62%] [G loss: 1.088202]\n",
            "227 [D loss: 1.102309, acc.: 59.38%, op_acc: 87.70%] [G loss: 1.186882]\n",
            "228 [D loss: 1.018677, acc.: 58.98%, op_acc: 89.26%] [G loss: 1.129366]\n",
            "229 [D loss: 1.106945, acc.: 59.18%, op_acc: 86.52%] [G loss: 1.205099]\n",
            "230 [D loss: 1.028950, acc.: 61.33%, op_acc: 88.09%] [G loss: 1.140631]\n",
            "231 [D loss: 1.111923, acc.: 51.37%, op_acc: 88.87%] [G loss: 1.098771]\n",
            "232 [D loss: 1.159117, acc.: 49.41%, op_acc: 88.48%] [G loss: 1.053010]\n",
            "233 [D loss: 1.078374, acc.: 57.81%, op_acc: 85.94%] [G loss: 1.141832]\n",
            "234 [D loss: 0.989823, acc.: 59.57%, op_acc: 90.82%] [G loss: 1.186890]\n",
            "235 [D loss: 1.042879, acc.: 60.16%, op_acc: 88.28%] [G loss: 1.189162]\n",
            "236 [D loss: 0.971208, acc.: 70.90%, op_acc: 90.04%] [G loss: 1.232755]\n",
            "237 [D loss: 1.026765, acc.: 61.91%, op_acc: 89.45%] [G loss: 1.114126]\n",
            "238 [D loss: 0.993700, acc.: 61.52%, op_acc: 90.62%] [G loss: 1.117654]\n",
            "239 [D loss: 1.076783, acc.: 56.45%, op_acc: 88.48%] [G loss: 1.091376]\n",
            "240 [D loss: 1.096356, acc.: 52.34%, op_acc: 89.26%] [G loss: 1.083787]\n",
            "241 [D loss: 1.148981, acc.: 47.46%, op_acc: 87.50%] [G loss: 0.985354]\n",
            "242 [D loss: 1.090588, acc.: 48.05%, op_acc: 90.82%] [G loss: 0.992707]\n",
            "243 [D loss: 1.117298, acc.: 48.24%, op_acc: 90.43%] [G loss: 1.021337]\n",
            "244 [D loss: 1.218572, acc.: 50.59%, op_acc: 87.30%] [G loss: 1.137605]\n",
            "245 [D loss: 1.075878, acc.: 55.47%, op_acc: 89.65%] [G loss: 1.109712]\n",
            "246 [D loss: 1.039411, acc.: 59.18%, op_acc: 89.06%] [G loss: 1.103889]\n",
            "247 [D loss: 1.017578, acc.: 63.87%, op_acc: 87.30%] [G loss: 1.143413]\n",
            "248 [D loss: 0.997098, acc.: 67.38%, op_acc: 87.30%] [G loss: 1.145462]\n",
            "249 [D loss: 1.011082, acc.: 67.58%, op_acc: 88.48%] [G loss: 1.173742]\n",
            "250 [D loss: 0.950981, acc.: 70.90%, op_acc: 88.48%] [G loss: 1.120788]\n",
            "251 [D loss: 0.945062, acc.: 69.73%, op_acc: 85.94%] [G loss: 1.163779]\n",
            "252 [D loss: 0.877462, acc.: 75.78%, op_acc: 87.89%] [G loss: 1.194294]\n",
            "253 [D loss: 0.864322, acc.: 71.09%, op_acc: 91.60%] [G loss: 1.227344]\n",
            "254 [D loss: 0.815529, acc.: 79.69%, op_acc: 90.82%] [G loss: 1.203555]\n",
            "255 [D loss: 0.837964, acc.: 68.75%, op_acc: 91.60%] [G loss: 1.066097]\n",
            "256 [D loss: 1.002691, acc.: 68.16%, op_acc: 89.06%] [G loss: 1.086900]\n",
            "257 [D loss: 1.021438, acc.: 60.74%, op_acc: 91.21%] [G loss: 0.959263]\n",
            "258 [D loss: 1.123040, acc.: 51.76%, op_acc: 89.84%] [G loss: 1.021479]\n",
            "259 [D loss: 1.097457, acc.: 51.76%, op_acc: 89.65%] [G loss: 0.969936]\n",
            "260 [D loss: 1.073102, acc.: 47.07%, op_acc: 88.87%] [G loss: 1.034194]\n",
            "261 [D loss: 0.996276, acc.: 58.59%, op_acc: 90.82%] [G loss: 0.953494]\n",
            "262 [D loss: 1.130988, acc.: 55.08%, op_acc: 88.48%] [G loss: 1.032401]\n",
            "263 [D loss: 0.985591, acc.: 56.05%, op_acc: 91.60%] [G loss: 1.014691]\n",
            "264 [D loss: 1.081082, acc.: 60.94%, op_acc: 88.28%] [G loss: 1.131058]\n",
            "265 [D loss: 1.003795, acc.: 65.23%, op_acc: 90.04%] [G loss: 1.142618]\n",
            "266 [D loss: 0.983682, acc.: 61.33%, op_acc: 89.06%] [G loss: 1.161382]\n",
            "267 [D loss: 0.986800, acc.: 58.40%, op_acc: 89.26%] [G loss: 1.203989]\n",
            "268 [D loss: 0.944496, acc.: 67.38%, op_acc: 88.09%] [G loss: 1.133264]\n",
            "269 [D loss: 1.028148, acc.: 55.47%, op_acc: 91.41%] [G loss: 1.033667]\n",
            "270 [D loss: 1.023929, acc.: 57.62%, op_acc: 89.06%] [G loss: 0.992970]\n",
            "271 [D loss: 1.075772, acc.: 52.34%, op_acc: 88.48%] [G loss: 0.976989]\n",
            "272 [D loss: 1.086646, acc.: 49.61%, op_acc: 90.04%] [G loss: 1.024091]\n",
            "273 [D loss: 1.153906, acc.: 47.85%, op_acc: 88.09%] [G loss: 1.051171]\n",
            "274 [D loss: 1.164773, acc.: 43.55%, op_acc: 89.45%] [G loss: 1.042466]\n",
            "275 [D loss: 1.192494, acc.: 43.55%, op_acc: 88.48%] [G loss: 1.033927]\n",
            "276 [D loss: 1.039678, acc.: 51.76%, op_acc: 91.02%] [G loss: 1.103204]\n",
            "277 [D loss: 1.002922, acc.: 59.77%, op_acc: 91.02%] [G loss: 1.059943]\n",
            "278 [D loss: 1.118189, acc.: 54.30%, op_acc: 89.06%] [G loss: 1.020999]\n",
            "279 [D loss: 1.084757, acc.: 50.98%, op_acc: 89.65%] [G loss: 1.014578]\n",
            "280 [D loss: 1.059386, acc.: 57.23%, op_acc: 88.28%] [G loss: 0.987871]\n",
            "281 [D loss: 1.002958, acc.: 58.59%, op_acc: 88.48%] [G loss: 0.979410]\n",
            "282 [D loss: 1.050604, acc.: 61.13%, op_acc: 89.06%] [G loss: 1.009407]\n",
            "283 [D loss: 0.842584, acc.: 76.95%, op_acc: 89.26%] [G loss: 1.046595]\n",
            "284 [D loss: 0.940738, acc.: 71.88%, op_acc: 90.82%] [G loss: 1.056761]\n",
            "285 [D loss: 0.807289, acc.: 74.02%, op_acc: 93.36%] [G loss: 1.048909]\n",
            "286 [D loss: 0.870001, acc.: 77.73%, op_acc: 91.21%] [G loss: 1.021024]\n",
            "287 [D loss: 0.857852, acc.: 80.08%, op_acc: 89.45%] [G loss: 1.093197]\n",
            "288 [D loss: 0.902516, acc.: 71.48%, op_acc: 89.65%] [G loss: 0.985788]\n",
            "289 [D loss: 0.935186, acc.: 68.16%, op_acc: 90.04%] [G loss: 1.007122]\n",
            "290 [D loss: 0.989502, acc.: 57.62%, op_acc: 91.99%] [G loss: 1.057145]\n",
            "291 [D loss: 0.902585, acc.: 66.21%, op_acc: 89.84%] [G loss: 1.025224]\n",
            "292 [D loss: 1.042340, acc.: 53.32%, op_acc: 91.02%] [G loss: 1.078856]\n",
            "293 [D loss: 0.954709, acc.: 59.96%, op_acc: 91.60%] [G loss: 1.151423]\n",
            "294 [D loss: 0.932703, acc.: 64.06%, op_acc: 92.19%] [G loss: 1.133488]\n",
            "295 [D loss: 1.021676, acc.: 56.05%, op_acc: 90.23%] [G loss: 1.199925]\n",
            "296 [D loss: 1.078857, acc.: 56.64%, op_acc: 90.23%] [G loss: 1.149783]\n",
            "297 [D loss: 1.099885, acc.: 47.27%, op_acc: 90.23%] [G loss: 1.103047]\n",
            "298 [D loss: 1.026983, acc.: 54.30%, op_acc: 92.19%] [G loss: 1.048615]\n",
            "299 [D loss: 1.090909, acc.: 47.46%, op_acc: 90.62%] [G loss: 1.090208]\n",
            "300 [D loss: 1.202449, acc.: 40.43%, op_acc: 90.23%] [G loss: 1.056249]\n",
            "301 [D loss: 1.086011, acc.: 42.19%, op_acc: 91.80%] [G loss: 1.031000]\n",
            "302 [D loss: 1.127414, acc.: 42.77%, op_acc: 91.02%] [G loss: 1.034120]\n",
            "303 [D loss: 1.167935, acc.: 44.14%, op_acc: 90.04%] [G loss: 1.065238]\n",
            "304 [D loss: 1.072906, acc.: 48.83%, op_acc: 91.02%] [G loss: 1.053230]\n",
            "305 [D loss: 1.092452, acc.: 48.44%, op_acc: 89.84%] [G loss: 1.055247]\n",
            "306 [D loss: 1.030828, acc.: 58.98%, op_acc: 89.06%] [G loss: 1.052855]\n",
            "307 [D loss: 0.948952, acc.: 64.84%, op_acc: 90.43%] [G loss: 1.032267]\n",
            "308 [D loss: 0.981363, acc.: 59.38%, op_acc: 91.21%] [G loss: 1.032203]\n",
            "309 [D loss: 1.007834, acc.: 57.03%, op_acc: 90.04%] [G loss: 1.002387]\n",
            "310 [D loss: 0.993278, acc.: 60.55%, op_acc: 91.41%] [G loss: 1.024343]\n",
            "311 [D loss: 0.908809, acc.: 63.67%, op_acc: 91.21%] [G loss: 0.956603]\n",
            "312 [D loss: 0.973168, acc.: 63.87%, op_acc: 89.84%] [G loss: 1.097010]\n",
            "313 [D loss: 0.864763, acc.: 69.14%, op_acc: 91.41%] [G loss: 1.098339]\n",
            "314 [D loss: 0.865004, acc.: 65.23%, op_acc: 93.36%] [G loss: 0.995108]\n",
            "315 [D loss: 0.859895, acc.: 71.09%, op_acc: 91.60%] [G loss: 1.010269]\n",
            "316 [D loss: 0.942863, acc.: 71.48%, op_acc: 88.28%] [G loss: 1.020260]\n",
            "317 [D loss: 0.872665, acc.: 69.92%, op_acc: 91.21%] [G loss: 1.068191]\n",
            "318 [D loss: 0.941911, acc.: 66.99%, op_acc: 90.23%] [G loss: 1.060298]\n",
            "319 [D loss: 0.969294, acc.: 67.58%, op_acc: 89.84%] [G loss: 1.107578]\n",
            "320 [D loss: 0.992471, acc.: 56.05%, op_acc: 91.60%] [G loss: 1.079774]\n",
            "321 [D loss: 1.122526, acc.: 51.56%, op_acc: 89.84%] [G loss: 1.043298]\n",
            "322 [D loss: 0.909457, acc.: 60.35%, op_acc: 91.21%] [G loss: 1.105744]\n",
            "323 [D loss: 1.016175, acc.: 51.17%, op_acc: 91.41%] [G loss: 1.072234]\n",
            "324 [D loss: 0.992288, acc.: 54.49%, op_acc: 92.58%] [G loss: 1.081929]\n",
            "325 [D loss: 0.986500, acc.: 55.86%, op_acc: 92.19%] [G loss: 1.029573]\n",
            "326 [D loss: 1.001557, acc.: 66.02%, op_acc: 87.50%] [G loss: 1.098101]\n",
            "327 [D loss: 1.052060, acc.: 56.84%, op_acc: 89.06%] [G loss: 1.018401]\n",
            "328 [D loss: 0.987544, acc.: 62.70%, op_acc: 89.84%] [G loss: 0.964857]\n",
            "329 [D loss: 0.962760, acc.: 61.33%, op_acc: 90.23%] [G loss: 0.980977]\n",
            "330 [D loss: 0.978821, acc.: 58.79%, op_acc: 91.60%] [G loss: 0.941362]\n",
            "331 [D loss: 0.968489, acc.: 62.70%, op_acc: 90.43%] [G loss: 0.950822]\n",
            "332 [D loss: 0.905684, acc.: 61.72%, op_acc: 92.19%] [G loss: 0.932469]\n",
            "333 [D loss: 0.895551, acc.: 68.55%, op_acc: 90.82%] [G loss: 1.001891]\n",
            "334 [D loss: 0.936910, acc.: 67.97%, op_acc: 88.48%] [G loss: 1.100120]\n",
            "335 [D loss: 1.029753, acc.: 66.60%, op_acc: 89.26%] [G loss: 1.103200]\n",
            "336 [D loss: 1.034579, acc.: 60.16%, op_acc: 89.84%] [G loss: 1.042727]\n",
            "337 [D loss: 0.980244, acc.: 55.27%, op_acc: 91.80%] [G loss: 1.095957]\n",
            "338 [D loss: 1.018252, acc.: 55.66%, op_acc: 89.06%] [G loss: 1.028429]\n",
            "339 [D loss: 1.116709, acc.: 46.68%, op_acc: 90.23%] [G loss: 1.028161]\n",
            "340 [D loss: 1.094714, acc.: 43.16%, op_acc: 91.80%] [G loss: 1.046521]\n",
            "341 [D loss: 1.066231, acc.: 49.61%, op_acc: 91.02%] [G loss: 1.055287]\n",
            "342 [D loss: 0.966527, acc.: 54.10%, op_acc: 93.55%] [G loss: 1.111926]\n",
            "343 [D loss: 1.135934, acc.: 42.97%, op_acc: 91.80%] [G loss: 1.042513]\n",
            "344 [D loss: 1.034206, acc.: 48.44%, op_acc: 92.38%] [G loss: 1.043703]\n",
            "345 [D loss: 0.984680, acc.: 60.16%, op_acc: 91.21%] [G loss: 1.111779]\n",
            "346 [D loss: 0.976329, acc.: 60.74%, op_acc: 89.45%] [G loss: 1.112664]\n",
            "347 [D loss: 0.823086, acc.: 70.12%, op_acc: 92.58%] [G loss: 1.004501]\n",
            "348 [D loss: 0.881221, acc.: 65.82%, op_acc: 91.41%] [G loss: 1.030300]\n",
            "349 [D loss: 0.951004, acc.: 64.26%, op_acc: 91.02%] [G loss: 0.989184]\n",
            "350 [D loss: 0.872456, acc.: 65.62%, op_acc: 92.97%] [G loss: 0.897878]\n",
            "351 [D loss: 0.908510, acc.: 65.23%, op_acc: 93.16%] [G loss: 0.998829]\n",
            "352 [D loss: 0.964102, acc.: 60.74%, op_acc: 91.21%] [G loss: 1.105193]\n",
            "353 [D loss: 0.835609, acc.: 67.97%, op_acc: 93.75%] [G loss: 1.062093]\n",
            "354 [D loss: 0.971715, acc.: 60.35%, op_acc: 89.65%] [G loss: 1.140692]\n",
            "355 [D loss: 0.940837, acc.: 62.11%, op_acc: 90.62%] [G loss: 1.129193]\n",
            "356 [D loss: 1.023003, acc.: 57.62%, op_acc: 89.06%] [G loss: 1.062642]\n",
            "357 [D loss: 1.003425, acc.: 52.73%, op_acc: 91.60%] [G loss: 1.061801]\n",
            "358 [D loss: 1.032830, acc.: 47.27%, op_acc: 91.21%] [G loss: 1.123770]\n",
            "359 [D loss: 1.060841, acc.: 50.00%, op_acc: 90.82%] [G loss: 1.087388]\n",
            "360 [D loss: 0.942691, acc.: 56.25%, op_acc: 92.77%] [G loss: 1.142337]\n",
            "361 [D loss: 1.001668, acc.: 55.27%, op_acc: 92.38%] [G loss: 1.082018]\n",
            "362 [D loss: 1.128827, acc.: 53.32%, op_acc: 88.67%] [G loss: 1.041213]\n",
            "363 [D loss: 0.960875, acc.: 59.38%, op_acc: 90.04%] [G loss: 1.047713]\n",
            "364 [D loss: 0.948881, acc.: 54.49%, op_acc: 91.41%] [G loss: 0.954811]\n",
            "365 [D loss: 0.925293, acc.: 62.11%, op_acc: 92.19%] [G loss: 1.029068]\n",
            "366 [D loss: 0.792454, acc.: 74.41%, op_acc: 91.99%] [G loss: 1.030556]\n",
            "367 [D loss: 0.803713, acc.: 72.27%, op_acc: 92.58%] [G loss: 1.042067]\n",
            "368 [D loss: 0.819416, acc.: 74.02%, op_acc: 91.60%] [G loss: 0.951494]\n",
            "369 [D loss: 0.770981, acc.: 74.80%, op_acc: 91.80%] [G loss: 0.959483]\n",
            "370 [D loss: 0.825712, acc.: 76.76%, op_acc: 92.19%] [G loss: 1.004942]\n",
            "371 [D loss: 0.898878, acc.: 73.24%, op_acc: 90.04%] [G loss: 1.042762]\n",
            "372 [D loss: 0.734554, acc.: 82.23%, op_acc: 90.04%] [G loss: 1.117714]\n",
            "373 [D loss: 0.686950, acc.: 88.67%, op_acc: 90.43%] [G loss: 1.188948]\n",
            "374 [D loss: 0.726408, acc.: 85.55%, op_acc: 90.62%] [G loss: 1.243544]\n",
            "375 [D loss: 0.842584, acc.: 73.83%, op_acc: 87.89%] [G loss: 1.238151]\n",
            "376 [D loss: 0.810453, acc.: 81.05%, op_acc: 91.02%] [G loss: 1.151516]\n",
            "377 [D loss: 0.788255, acc.: 75.20%, op_acc: 91.41%] [G loss: 1.192471]\n",
            "378 [D loss: 0.943864, acc.: 61.13%, op_acc: 91.60%] [G loss: 1.149781]\n",
            "379 [D loss: 1.025428, acc.: 53.12%, op_acc: 91.60%] [G loss: 1.209923]\n",
            "380 [D loss: 1.134509, acc.: 44.14%, op_acc: 91.60%] [G loss: 1.212075]\n",
            "381 [D loss: 1.048974, acc.: 52.73%, op_acc: 92.38%] [G loss: 1.246957]\n",
            "382 [D loss: 1.096683, acc.: 45.70%, op_acc: 91.21%] [G loss: 1.172131]\n",
            "383 [D loss: 1.076060, acc.: 50.00%, op_acc: 91.21%] [G loss: 1.163306]\n",
            "384 [D loss: 1.078626, acc.: 52.93%, op_acc: 90.62%] [G loss: 1.125667]\n",
            "385 [D loss: 0.898951, acc.: 60.35%, op_acc: 93.36%] [G loss: 1.039369]\n",
            "386 [D loss: 0.868674, acc.: 66.99%, op_acc: 91.21%] [G loss: 0.988848]\n",
            "387 [D loss: 0.937267, acc.: 60.16%, op_acc: 92.38%] [G loss: 0.843570]\n",
            "388 [D loss: 0.884708, acc.: 65.43%, op_acc: 92.38%] [G loss: 0.815396]\n",
            "389 [D loss: 0.882078, acc.: 70.12%, op_acc: 90.62%] [G loss: 0.876603]\n",
            "390 [D loss: 0.786912, acc.: 77.34%, op_acc: 92.19%] [G loss: 0.934094]\n",
            "391 [D loss: 0.854306, acc.: 75.98%, op_acc: 90.23%] [G loss: 1.075889]\n",
            "392 [D loss: 0.772451, acc.: 79.10%, op_acc: 90.62%] [G loss: 1.201700]\n",
            "393 [D loss: 0.842077, acc.: 73.24%, op_acc: 91.02%] [G loss: 1.271380]\n",
            "394 [D loss: 0.808866, acc.: 78.71%, op_acc: 90.43%] [G loss: 1.269183]\n",
            "395 [D loss: 0.758120, acc.: 81.05%, op_acc: 91.21%] [G loss: 1.254042]\n",
            "396 [D loss: 0.713964, acc.: 85.35%, op_acc: 91.02%] [G loss: 1.283837]\n",
            "397 [D loss: 0.862709, acc.: 69.14%, op_acc: 90.82%] [G loss: 1.153881]\n",
            "398 [D loss: 0.733262, acc.: 73.83%, op_acc: 94.73%] [G loss: 1.146454]\n",
            "399 [D loss: 1.006630, acc.: 55.66%, op_acc: 92.19%] [G loss: 1.164311]\n",
            "400 [D loss: 1.090221, acc.: 42.38%, op_acc: 91.21%] [G loss: 1.166702]\n",
            "401 [D loss: 1.105529, acc.: 42.97%, op_acc: 92.58%] [G loss: 1.095689]\n",
            "402 [D loss: 1.254671, acc.: 33.59%, op_acc: 90.43%] [G loss: 1.057979]\n",
            "403 [D loss: 1.201233, acc.: 35.35%, op_acc: 91.60%] [G loss: 1.021759]\n",
            "404 [D loss: 1.038630, acc.: 47.66%, op_acc: 92.38%] [G loss: 1.049738]\n",
            "405 [D loss: 1.023978, acc.: 50.78%, op_acc: 92.97%] [G loss: 1.053075]\n",
            "406 [D loss: 1.014220, acc.: 50.39%, op_acc: 91.80%] [G loss: 1.005252]\n",
            "407 [D loss: 0.883081, acc.: 62.50%, op_acc: 92.77%] [G loss: 0.964790]\n",
            "408 [D loss: 0.871882, acc.: 65.04%, op_acc: 91.99%] [G loss: 0.911075]\n",
            "409 [D loss: 0.821210, acc.: 78.12%, op_acc: 90.04%] [G loss: 0.884611]\n",
            "410 [D loss: 0.769066, acc.: 79.10%, op_acc: 90.62%] [G loss: 0.885395]\n",
            "411 [D loss: 0.699285, acc.: 84.57%, op_acc: 91.99%] [G loss: 0.889636]\n",
            "412 [D loss: 0.754587, acc.: 84.38%, op_acc: 91.80%] [G loss: 0.874898]\n",
            "413 [D loss: 0.634813, acc.: 89.65%, op_acc: 93.36%] [G loss: 0.969462]\n",
            "414 [D loss: 0.660278, acc.: 91.60%, op_acc: 91.21%] [G loss: 0.984663]\n",
            "415 [D loss: 0.679599, acc.: 94.73%, op_acc: 88.87%] [G loss: 1.131484]\n",
            "416 [D loss: 0.637828, acc.: 93.95%, op_acc: 91.21%] [G loss: 1.094374]\n",
            "417 [D loss: 0.577099, acc.: 93.95%, op_acc: 91.60%] [G loss: 1.193411]\n",
            "418 [D loss: 0.653154, acc.: 87.11%, op_acc: 91.80%] [G loss: 1.121152]\n",
            "419 [D loss: 0.756188, acc.: 82.03%, op_acc: 90.82%] [G loss: 1.242754]\n",
            "420 [D loss: 0.756236, acc.: 75.39%, op_acc: 91.99%] [G loss: 1.196037]\n",
            "421 [D loss: 0.947006, acc.: 57.42%, op_acc: 91.60%] [G loss: 1.101787]\n",
            "422 [D loss: 0.937298, acc.: 61.52%, op_acc: 91.60%] [G loss: 1.135702]\n",
            "423 [D loss: 1.100167, acc.: 42.97%, op_acc: 92.58%] [G loss: 1.148007]\n",
            "424 [D loss: 1.365665, acc.: 33.20%, op_acc: 87.89%] [G loss: 1.121963]\n",
            "425 [D loss: 1.273232, acc.: 33.20%, op_acc: 91.02%] [G loss: 1.178797]\n",
            "426 [D loss: 1.208743, acc.: 37.30%, op_acc: 89.84%] [G loss: 1.107217]\n",
            "427 [D loss: 1.319047, acc.: 28.52%, op_acc: 91.80%] [G loss: 1.097846]\n",
            "428 [D loss: 1.348927, acc.: 27.15%, op_acc: 89.45%] [G loss: 1.109861]\n",
            "429 [D loss: 1.269465, acc.: 32.62%, op_acc: 90.62%] [G loss: 1.097394]\n",
            "430 [D loss: 1.123466, acc.: 43.75%, op_acc: 90.62%] [G loss: 1.052629]\n",
            "431 [D loss: 0.991108, acc.: 46.48%, op_acc: 93.36%] [G loss: 1.052741]\n",
            "432 [D loss: 0.969666, acc.: 56.84%, op_acc: 91.41%] [G loss: 0.964039]\n",
            "433 [D loss: 0.811547, acc.: 69.53%, op_acc: 92.77%] [G loss: 0.978437]\n",
            "434 [D loss: 0.821862, acc.: 75.20%, op_acc: 91.60%] [G loss: 0.949535]\n",
            "435 [D loss: 0.787698, acc.: 75.39%, op_acc: 92.77%] [G loss: 1.006261]\n",
            "436 [D loss: 0.800227, acc.: 80.27%, op_acc: 91.41%] [G loss: 0.996102]\n",
            "437 [D loss: 0.696555, acc.: 82.81%, op_acc: 91.99%] [G loss: 1.026841]\n",
            "438 [D loss: 0.729279, acc.: 83.98%, op_acc: 91.21%] [G loss: 0.991288]\n",
            "439 [D loss: 0.761624, acc.: 83.01%, op_acc: 92.38%] [G loss: 0.984339]\n",
            "440 [D loss: 0.708417, acc.: 83.98%, op_acc: 92.38%] [G loss: 1.017635]\n",
            "441 [D loss: 0.776018, acc.: 79.49%, op_acc: 90.82%] [G loss: 1.032995]\n",
            "442 [D loss: 0.724363, acc.: 76.95%, op_acc: 93.55%] [G loss: 0.992320]\n",
            "443 [D loss: 0.855358, acc.: 76.37%, op_acc: 92.58%] [G loss: 0.925126]\n",
            "444 [D loss: 0.879393, acc.: 67.77%, op_acc: 91.41%] [G loss: 1.006107]\n",
            "445 [D loss: 0.817506, acc.: 69.14%, op_acc: 92.58%] [G loss: 0.948147]\n",
            "446 [D loss: 0.893429, acc.: 68.75%, op_acc: 91.99%] [G loss: 1.058773]\n",
            "447 [D loss: 0.908466, acc.: 63.09%, op_acc: 92.97%] [G loss: 1.103528]\n",
            "448 [D loss: 0.925262, acc.: 67.58%, op_acc: 90.04%] [G loss: 1.043182]\n",
            "449 [D loss: 0.938850, acc.: 59.57%, op_acc: 90.43%] [G loss: 1.028663]\n",
            "450 [D loss: 0.984665, acc.: 63.09%, op_acc: 89.45%] [G loss: 1.057290]\n",
            "451 [D loss: 0.766348, acc.: 71.09%, op_acc: 94.53%] [G loss: 1.061217]\n",
            "452 [D loss: 0.868584, acc.: 62.30%, op_acc: 92.77%] [G loss: 1.055770]\n",
            "453 [D loss: 0.958619, acc.: 53.12%, op_acc: 92.19%] [G loss: 0.995606]\n",
            "454 [D loss: 0.970719, acc.: 60.16%, op_acc: 92.19%] [G loss: 0.997194]\n",
            "455 [D loss: 0.946382, acc.: 59.38%, op_acc: 92.58%] [G loss: 0.954514]\n",
            "456 [D loss: 0.893561, acc.: 63.87%, op_acc: 92.97%] [G loss: 0.968095]\n",
            "457 [D loss: 0.922247, acc.: 62.11%, op_acc: 91.21%] [G loss: 1.026770]\n",
            "458 [D loss: 0.983109, acc.: 58.98%, op_acc: 91.41%] [G loss: 1.064583]\n",
            "459 [D loss: 1.059289, acc.: 47.07%, op_acc: 92.19%] [G loss: 1.030055]\n",
            "460 [D loss: 1.032643, acc.: 55.66%, op_acc: 90.82%] [G loss: 1.052606]\n",
            "461 [D loss: 1.080537, acc.: 44.92%, op_acc: 91.02%] [G loss: 1.033869]\n",
            "462 [D loss: 1.030868, acc.: 48.24%, op_acc: 93.75%] [G loss: 1.039214]\n",
            "463 [D loss: 1.081668, acc.: 42.38%, op_acc: 92.38%] [G loss: 0.968946]\n",
            "464 [D loss: 1.047579, acc.: 51.37%, op_acc: 92.77%] [G loss: 1.057847]\n",
            "465 [D loss: 1.099744, acc.: 54.88%, op_acc: 90.23%] [G loss: 1.009411]\n",
            "466 [D loss: 1.040161, acc.: 45.90%, op_acc: 92.58%] [G loss: 0.993075]\n",
            "467 [D loss: 0.984909, acc.: 59.77%, op_acc: 92.19%] [G loss: 1.028838]\n",
            "468 [D loss: 1.021255, acc.: 48.63%, op_acc: 93.16%] [G loss: 0.930500]\n",
            "469 [D loss: 0.962368, acc.: 57.81%, op_acc: 92.58%] [G loss: 0.949576]\n",
            "470 [D loss: 0.922850, acc.: 60.74%, op_acc: 92.77%] [G loss: 0.882183]\n",
            "471 [D loss: 0.863436, acc.: 62.30%, op_acc: 93.95%] [G loss: 0.950840]\n",
            "472 [D loss: 0.839019, acc.: 69.53%, op_acc: 90.43%] [G loss: 0.933470]\n",
            "473 [D loss: 0.811657, acc.: 72.66%, op_acc: 92.38%] [G loss: 1.000555]\n",
            "474 [D loss: 0.839025, acc.: 68.55%, op_acc: 93.55%] [G loss: 1.036323]\n",
            "475 [D loss: 0.690383, acc.: 81.25%, op_acc: 94.34%] [G loss: 1.015715]\n",
            "476 [D loss: 0.787674, acc.: 69.92%, op_acc: 93.55%] [G loss: 1.017851]\n",
            "477 [D loss: 0.711958, acc.: 77.93%, op_acc: 93.16%] [G loss: 1.065863]\n",
            "478 [D loss: 0.732742, acc.: 83.40%, op_acc: 91.80%] [G loss: 1.054744]\n",
            "479 [D loss: 0.704665, acc.: 80.08%, op_acc: 91.99%] [G loss: 1.055424]\n",
            "480 [D loss: 0.698176, acc.: 78.32%, op_acc: 93.36%] [G loss: 1.028816]\n",
            "481 [D loss: 0.779220, acc.: 75.78%, op_acc: 91.99%] [G loss: 0.983253]\n",
            "482 [D loss: 0.791358, acc.: 74.80%, op_acc: 91.99%] [G loss: 1.021824]\n",
            "483 [D loss: 0.831879, acc.: 73.83%, op_acc: 91.60%] [G loss: 1.034243]\n",
            "484 [D loss: 0.838042, acc.: 68.36%, op_acc: 92.97%] [G loss: 1.072100]\n",
            "485 [D loss: 0.858471, acc.: 72.07%, op_acc: 91.02%] [G loss: 1.073259]\n",
            "486 [D loss: 0.884336, acc.: 57.03%, op_acc: 93.95%] [G loss: 1.006629]\n",
            "487 [D loss: 1.105103, acc.: 49.61%, op_acc: 92.19%] [G loss: 1.017077]\n",
            "488 [D loss: 1.189631, acc.: 32.42%, op_acc: 92.77%] [G loss: 1.009399]\n",
            "489 [D loss: 1.066995, acc.: 49.41%, op_acc: 90.82%] [G loss: 1.016367]\n",
            "490 [D loss: 1.097237, acc.: 40.82%, op_acc: 92.77%] [G loss: 1.041297]\n",
            "491 [D loss: 1.100452, acc.: 39.84%, op_acc: 91.21%] [G loss: 0.983428]\n",
            "492 [D loss: 1.041795, acc.: 45.12%, op_acc: 93.55%] [G loss: 0.928660]\n",
            "493 [D loss: 1.022577, acc.: 39.26%, op_acc: 95.51%] [G loss: 0.866317]\n",
            "494 [D loss: 1.054862, acc.: 41.80%, op_acc: 91.41%] [G loss: 0.866576]\n",
            "495 [D loss: 1.004698, acc.: 50.78%, op_acc: 92.58%] [G loss: 0.888096]\n",
            "496 [D loss: 0.884976, acc.: 64.06%, op_acc: 92.19%] [G loss: 0.962169]\n",
            "497 [D loss: 0.859080, acc.: 67.38%, op_acc: 93.55%] [G loss: 0.975203]\n",
            "498 [D loss: 0.845558, acc.: 69.92%, op_acc: 92.38%] [G loss: 0.986878]\n",
            "499 [D loss: 0.777069, acc.: 73.44%, op_acc: 93.16%] [G loss: 0.956364]\n",
            "500 [D loss: 0.721924, acc.: 78.12%, op_acc: 93.36%] [G loss: 0.993319]\n",
            "501 [D loss: 0.657439, acc.: 84.57%, op_acc: 93.16%] [G loss: 0.964862]\n",
            "502 [D loss: 0.713895, acc.: 85.16%, op_acc: 91.60%] [G loss: 1.008020]\n",
            "503 [D loss: 0.545302, acc.: 92.38%, op_acc: 93.36%] [G loss: 1.017159]\n",
            "504 [D loss: 0.579882, acc.: 91.60%, op_acc: 92.97%] [G loss: 0.974858]\n",
            "505 [D loss: 0.561982, acc.: 95.51%, op_acc: 93.36%] [G loss: 0.971994]\n",
            "506 [D loss: 0.620911, acc.: 90.82%, op_acc: 92.77%] [G loss: 1.014955]\n",
            "507 [D loss: 0.544176, acc.: 94.53%, op_acc: 92.77%] [G loss: 1.082265]\n",
            "508 [D loss: 0.636622, acc.: 87.11%, op_acc: 91.80%] [G loss: 1.106742]\n",
            "509 [D loss: 0.695079, acc.: 79.10%, op_acc: 91.41%] [G loss: 1.113216]\n",
            "510 [D loss: 0.614941, acc.: 82.81%, op_acc: 95.31%] [G loss: 1.162238]\n",
            "511 [D loss: 0.829153, acc.: 67.77%, op_acc: 91.41%] [G loss: 1.107926]\n",
            "512 [D loss: 0.963751, acc.: 56.64%, op_acc: 92.58%] [G loss: 1.101394]\n",
            "513 [D loss: 0.987570, acc.: 52.93%, op_acc: 93.95%] [G loss: 1.160625]\n",
            "514 [D loss: 0.987675, acc.: 54.30%, op_acc: 91.60%] [G loss: 1.161668]\n",
            "515 [D loss: 1.090252, acc.: 45.90%, op_acc: 90.62%] [G loss: 1.035603]\n",
            "516 [D loss: 0.977956, acc.: 56.84%, op_acc: 92.58%] [G loss: 1.033439]\n",
            "517 [D loss: 1.124233, acc.: 42.19%, op_acc: 91.99%] [G loss: 1.022187]\n",
            "518 [D loss: 0.869820, acc.: 60.94%, op_acc: 93.16%] [G loss: 0.998039]\n",
            "519 [D loss: 0.934904, acc.: 57.42%, op_acc: 92.38%] [G loss: 0.977560]\n",
            "520 [D loss: 0.957955, acc.: 62.30%, op_acc: 90.43%] [G loss: 1.001291]\n",
            "521 [D loss: 0.933372, acc.: 58.20%, op_acc: 93.55%] [G loss: 1.024300]\n",
            "522 [D loss: 0.864784, acc.: 61.13%, op_acc: 91.41%] [G loss: 0.986043]\n",
            "523 [D loss: 0.910988, acc.: 61.91%, op_acc: 92.97%] [G loss: 0.972290]\n",
            "524 [D loss: 0.906636, acc.: 58.20%, op_acc: 93.55%] [G loss: 0.950708]\n",
            "525 [D loss: 0.981463, acc.: 58.79%, op_acc: 92.77%] [G loss: 0.945541]\n",
            "526 [D loss: 0.841936, acc.: 70.51%, op_acc: 91.60%] [G loss: 1.074346]\n",
            "527 [D loss: 0.854419, acc.: 69.53%, op_acc: 91.41%] [G loss: 1.077571]\n",
            "528 [D loss: 0.990440, acc.: 53.91%, op_acc: 93.55%] [G loss: 1.028217]\n",
            "529 [D loss: 0.938087, acc.: 58.79%, op_acc: 91.99%] [G loss: 0.995094]\n",
            "530 [D loss: 0.854192, acc.: 66.60%, op_acc: 91.99%] [G loss: 1.030202]\n",
            "531 [D loss: 0.846576, acc.: 67.58%, op_acc: 91.60%] [G loss: 0.988198]\n",
            "532 [D loss: 0.924933, acc.: 64.45%, op_acc: 90.62%] [G loss: 0.985301]\n",
            "533 [D loss: 0.750916, acc.: 74.80%, op_acc: 93.75%] [G loss: 0.970379]\n",
            "534 [D loss: 0.814074, acc.: 66.41%, op_acc: 94.14%] [G loss: 0.988051]\n",
            "535 [D loss: 0.819026, acc.: 71.48%, op_acc: 92.77%] [G loss: 1.034512]\n",
            "536 [D loss: 0.954148, acc.: 50.78%, op_acc: 94.34%] [G loss: 0.998377]\n",
            "537 [D loss: 0.906871, acc.: 61.52%, op_acc: 92.58%] [G loss: 1.046247]\n",
            "538 [D loss: 0.987926, acc.: 51.95%, op_acc: 91.60%] [G loss: 0.946939]\n",
            "539 [D loss: 0.866732, acc.: 65.62%, op_acc: 92.38%] [G loss: 0.981450]\n",
            "540 [D loss: 0.963235, acc.: 56.25%, op_acc: 92.58%] [G loss: 0.925743]\n",
            "541 [D loss: 0.775030, acc.: 72.27%, op_acc: 92.77%] [G loss: 0.953707]\n",
            "542 [D loss: 0.672795, acc.: 83.40%, op_acc: 93.16%] [G loss: 0.964504]\n",
            "543 [D loss: 0.795145, acc.: 70.12%, op_acc: 94.14%] [G loss: 0.916278]\n",
            "544 [D loss: 0.814711, acc.: 74.22%, op_acc: 90.82%] [G loss: 0.960819]\n",
            "545 [D loss: 0.733183, acc.: 73.63%, op_acc: 94.34%] [G loss: 0.946709]\n",
            "546 [D loss: 0.717686, acc.: 77.15%, op_acc: 93.95%] [G loss: 1.018344]\n",
            "547 [D loss: 0.730717, acc.: 80.66%, op_acc: 93.36%] [G loss: 1.055355]\n",
            "548 [D loss: 0.880793, acc.: 64.65%, op_acc: 92.58%] [G loss: 1.042588]\n",
            "549 [D loss: 0.794681, acc.: 65.23%, op_acc: 94.73%] [G loss: 1.067931]\n",
            "550 [D loss: 0.952776, acc.: 54.49%, op_acc: 92.77%] [G loss: 1.031618]\n",
            "551 [D loss: 0.984868, acc.: 53.71%, op_acc: 92.58%] [G loss: 1.107071]\n",
            "552 [D loss: 1.046975, acc.: 46.88%, op_acc: 93.55%] [G loss: 1.063736]\n",
            "553 [D loss: 1.177963, acc.: 32.81%, op_acc: 93.16%] [G loss: 1.012443]\n",
            "554 [D loss: 1.193491, acc.: 33.01%, op_acc: 92.58%] [G loss: 1.012733]\n",
            "555 [D loss: 1.095092, acc.: 41.60%, op_acc: 93.55%] [G loss: 0.984163]\n",
            "556 [D loss: 1.021202, acc.: 44.53%, op_acc: 94.14%] [G loss: 0.974502]\n",
            "557 [D loss: 1.000946, acc.: 53.52%, op_acc: 91.60%] [G loss: 0.866308]\n",
            "558 [D loss: 1.034474, acc.: 47.07%, op_acc: 92.97%] [G loss: 0.856061]\n",
            "559 [D loss: 0.918957, acc.: 59.77%, op_acc: 92.38%] [G loss: 0.825483]\n",
            "560 [D loss: 0.799866, acc.: 68.55%, op_acc: 92.38%] [G loss: 0.846465]\n",
            "561 [D loss: 0.653196, acc.: 83.40%, op_acc: 93.55%] [G loss: 0.913288]\n",
            "562 [D loss: 0.692529, acc.: 83.79%, op_acc: 91.41%] [G loss: 0.958992]\n",
            "563 [D loss: 0.708105, acc.: 84.57%, op_acc: 93.55%] [G loss: 0.945853]\n",
            "564 [D loss: 0.617405, acc.: 89.26%, op_acc: 91.60%] [G loss: 1.096272]\n",
            "565 [D loss: 0.509282, acc.: 96.68%, op_acc: 93.36%] [G loss: 1.018721]\n",
            "566 [D loss: 0.457700, acc.: 96.48%, op_acc: 93.75%] [G loss: 1.099082]\n",
            "567 [D loss: 0.566304, acc.: 90.62%, op_acc: 94.53%] [G loss: 1.138622]\n",
            "568 [D loss: 0.520269, acc.: 93.55%, op_acc: 94.53%] [G loss: 1.132399]\n",
            "569 [D loss: 0.544551, acc.: 96.48%, op_acc: 90.62%] [G loss: 1.135859]\n",
            "570 [D loss: 0.638916, acc.: 86.52%, op_acc: 92.97%] [G loss: 1.094350]\n",
            "571 [D loss: 0.616125, acc.: 91.02%, op_acc: 92.58%] [G loss: 1.098548]\n",
            "572 [D loss: 0.788611, acc.: 66.80%, op_acc: 94.14%] [G loss: 0.997116]\n",
            "573 [D loss: 0.807100, acc.: 68.75%, op_acc: 94.14%] [G loss: 1.034157]\n",
            "574 [D loss: 0.794437, acc.: 72.66%, op_acc: 93.16%] [G loss: 1.095381]\n",
            "575 [D loss: 1.029721, acc.: 50.98%, op_acc: 91.41%] [G loss: 1.047765]\n",
            "576 [D loss: 1.067083, acc.: 45.51%, op_acc: 93.16%] [G loss: 0.982491]\n",
            "577 [D loss: 0.975741, acc.: 50.98%, op_acc: 93.36%] [G loss: 1.004618]\n",
            "578 [D loss: 1.159261, acc.: 42.77%, op_acc: 90.62%] [G loss: 0.989461]\n",
            "579 [D loss: 1.146578, acc.: 41.60%, op_acc: 93.36%] [G loss: 0.989563]\n",
            "580 [D loss: 1.159116, acc.: 36.52%, op_acc: 93.16%] [G loss: 0.868315]\n",
            "581 [D loss: 1.010621, acc.: 47.85%, op_acc: 92.97%] [G loss: 0.815473]\n",
            "582 [D loss: 0.870224, acc.: 60.94%, op_acc: 93.55%] [G loss: 0.828108]\n",
            "583 [D loss: 0.926389, acc.: 56.05%, op_acc: 93.75%] [G loss: 0.850642]\n",
            "584 [D loss: 0.897308, acc.: 62.50%, op_acc: 93.36%] [G loss: 0.889266]\n",
            "585 [D loss: 0.784962, acc.: 73.44%, op_acc: 93.75%] [G loss: 0.874954]\n",
            "586 [D loss: 0.776668, acc.: 74.80%, op_acc: 91.60%] [G loss: 0.876546]\n",
            "587 [D loss: 0.704452, acc.: 76.76%, op_acc: 93.55%] [G loss: 0.938139]\n",
            "588 [D loss: 0.666442, acc.: 85.16%, op_acc: 93.55%] [G loss: 0.973112]\n",
            "589 [D loss: 0.790746, acc.: 77.15%, op_acc: 90.04%] [G loss: 0.960984]\n",
            "590 [D loss: 0.677357, acc.: 83.40%, op_acc: 94.53%] [G loss: 0.914156]\n",
            "591 [D loss: 0.780734, acc.: 74.80%, op_acc: 92.97%] [G loss: 1.005450]\n",
            "592 [D loss: 0.727550, acc.: 80.66%, op_acc: 92.38%] [G loss: 0.958597]\n",
            "593 [D loss: 0.800528, acc.: 70.12%, op_acc: 93.16%] [G loss: 0.992200]\n",
            "594 [D loss: 0.897485, acc.: 65.23%, op_acc: 91.99%] [G loss: 1.009227]\n",
            "595 [D loss: 0.813227, acc.: 72.85%, op_acc: 93.36%] [G loss: 0.949363]\n",
            "596 [D loss: 0.887035, acc.: 59.38%, op_acc: 94.34%] [G loss: 0.974688]\n",
            "597 [D loss: 0.913397, acc.: 59.96%, op_acc: 93.75%] [G loss: 0.933545]\n",
            "598 [D loss: 0.899919, acc.: 58.40%, op_acc: 94.53%] [G loss: 0.958335]\n",
            "599 [D loss: 1.029423, acc.: 40.04%, op_acc: 94.14%] [G loss: 0.930972]\n",
            "600 [D loss: 1.135396, acc.: 38.67%, op_acc: 94.34%] [G loss: 0.916596]\n",
            "601 [D loss: 1.013605, acc.: 46.48%, op_acc: 93.95%] [G loss: 0.993941]\n",
            "602 [D loss: 1.034095, acc.: 53.52%, op_acc: 88.48%] [G loss: 0.963503]\n",
            "603 [D loss: 0.900493, acc.: 60.35%, op_acc: 93.75%] [G loss: 0.993417]\n",
            "604 [D loss: 0.870853, acc.: 58.79%, op_acc: 94.73%] [G loss: 1.013686]\n",
            "605 [D loss: 0.908214, acc.: 58.20%, op_acc: 94.34%] [G loss: 0.996072]\n",
            "606 [D loss: 1.005186, acc.: 49.22%, op_acc: 92.58%] [G loss: 0.845957]\n",
            "607 [D loss: 0.914894, acc.: 53.91%, op_acc: 95.31%] [G loss: 0.877844]\n",
            "608 [D loss: 0.964633, acc.: 49.61%, op_acc: 93.36%] [G loss: 0.877847]\n",
            "609 [D loss: 0.931618, acc.: 57.62%, op_acc: 93.36%] [G loss: 0.872149]\n",
            "610 [D loss: 1.034468, acc.: 42.97%, op_acc: 93.55%] [G loss: 0.926841]\n",
            "611 [D loss: 0.821766, acc.: 64.65%, op_acc: 93.75%] [G loss: 0.938600]\n",
            "612 [D loss: 0.884921, acc.: 57.42%, op_acc: 93.55%] [G loss: 0.911320]\n",
            "613 [D loss: 0.955505, acc.: 47.85%, op_acc: 94.73%] [G loss: 0.880406]\n",
            "614 [D loss: 0.923024, acc.: 58.98%, op_acc: 92.77%] [G loss: 0.908423]\n",
            "615 [D loss: 0.861948, acc.: 66.02%, op_acc: 92.58%] [G loss: 0.896695]\n",
            "616 [D loss: 0.818768, acc.: 70.90%, op_acc: 93.75%] [G loss: 0.899876]\n",
            "617 [D loss: 0.906855, acc.: 64.45%, op_acc: 91.99%] [G loss: 0.928294]\n",
            "618 [D loss: 0.770349, acc.: 69.73%, op_acc: 94.92%] [G loss: 0.980675]\n",
            "619 [D loss: 0.723534, acc.: 76.56%, op_acc: 93.75%] [G loss: 0.927857]\n",
            "620 [D loss: 0.823504, acc.: 69.73%, op_acc: 92.38%] [G loss: 0.886233]\n",
            "621 [D loss: 0.685835, acc.: 81.84%, op_acc: 92.97%] [G loss: 0.961994]\n",
            "622 [D loss: 0.779408, acc.: 81.84%, op_acc: 91.41%] [G loss: 0.972331]\n",
            "623 [D loss: 0.625842, acc.: 83.79%, op_acc: 92.97%] [G loss: 0.954997]\n",
            "624 [D loss: 0.665100, acc.: 85.16%, op_acc: 91.80%] [G loss: 0.971329]\n",
            "625 [D loss: 0.628790, acc.: 81.64%, op_acc: 95.12%] [G loss: 0.999295]\n",
            "626 [D loss: 0.733838, acc.: 74.41%, op_acc: 94.14%] [G loss: 0.980010]\n",
            "627 [D loss: 0.751331, acc.: 77.93%, op_acc: 93.36%] [G loss: 0.983637]\n",
            "628 [D loss: 0.860975, acc.: 60.94%, op_acc: 94.73%] [G loss: 1.000645]\n",
            "629 [D loss: 1.068324, acc.: 37.89%, op_acc: 94.73%] [G loss: 1.007676]\n",
            "630 [D loss: 1.124187, acc.: 37.11%, op_acc: 93.16%] [G loss: 0.930034]\n",
            "631 [D loss: 1.374403, acc.: 18.55%, op_acc: 92.97%] [G loss: 0.973458]\n",
            "632 [D loss: 1.187516, acc.: 31.05%, op_acc: 93.75%] [G loss: 0.910541]\n",
            "633 [D loss: 1.154353, acc.: 28.52%, op_acc: 95.51%] [G loss: 0.923067]\n",
            "634 [D loss: 1.025045, acc.: 41.99%, op_acc: 94.34%] [G loss: 0.937581]\n",
            "635 [D loss: 1.153670, acc.: 28.52%, op_acc: 94.92%] [G loss: 0.928118]\n",
            "636 [D loss: 1.041371, acc.: 39.06%, op_acc: 94.53%] [G loss: 0.855474]\n",
            "637 [D loss: 1.121440, acc.: 37.50%, op_acc: 93.75%] [G loss: 0.881649]\n",
            "638 [D loss: 0.810848, acc.: 65.04%, op_acc: 93.16%] [G loss: 0.868433]\n",
            "639 [D loss: 0.703386, acc.: 76.56%, op_acc: 94.73%] [G loss: 0.842071]\n",
            "640 [D loss: 0.697466, acc.: 73.24%, op_acc: 94.34%] [G loss: 0.893137]\n",
            "641 [D loss: 0.796367, acc.: 76.76%, op_acc: 90.82%] [G loss: 0.895670]\n",
            "642 [D loss: 0.646331, acc.: 84.57%, op_acc: 93.75%] [G loss: 0.950528]\n",
            "643 [D loss: 0.551394, acc.: 87.30%, op_acc: 94.92%] [G loss: 0.902165]\n",
            "644 [D loss: 0.555901, acc.: 92.77%, op_acc: 94.53%] [G loss: 0.964095]\n",
            "645 [D loss: 0.515824, acc.: 93.75%, op_acc: 94.53%] [G loss: 0.936959]\n",
            "646 [D loss: 0.516292, acc.: 93.36%, op_acc: 92.97%] [G loss: 0.989201]\n",
            "647 [D loss: 0.510703, acc.: 98.63%, op_acc: 91.80%] [G loss: 1.026245]\n",
            "648 [D loss: 0.605525, acc.: 90.23%, op_acc: 93.95%] [G loss: 0.965812]\n",
            "649 [D loss: 0.617138, acc.: 90.04%, op_acc: 91.99%] [G loss: 0.980413]\n",
            "650 [D loss: 0.614343, acc.: 91.02%, op_acc: 92.58%] [G loss: 1.047631]\n",
            "651 [D loss: 0.548126, acc.: 92.19%, op_acc: 93.36%] [G loss: 1.050523]\n",
            "652 [D loss: 0.481996, acc.: 96.88%, op_acc: 93.75%] [G loss: 1.067838]\n",
            "653 [D loss: 0.811316, acc.: 72.07%, op_acc: 92.38%] [G loss: 0.987324]\n",
            "654 [D loss: 0.812035, acc.: 63.09%, op_acc: 93.95%] [G loss: 0.995737]\n",
            "655 [D loss: 0.934079, acc.: 56.64%, op_acc: 93.55%] [G loss: 0.997276]\n",
            "656 [D loss: 0.929098, acc.: 57.42%, op_acc: 93.16%] [G loss: 0.991655]\n",
            "657 [D loss: 1.079910, acc.: 41.99%, op_acc: 93.16%] [G loss: 0.993718]\n",
            "658 [D loss: 1.421336, acc.: 23.05%, op_acc: 91.99%] [G loss: 0.932059]\n",
            "659 [D loss: 1.305466, acc.: 21.29%, op_acc: 93.36%] [G loss: 0.942053]\n",
            "660 [D loss: 1.196833, acc.: 29.88%, op_acc: 93.55%] [G loss: 0.944126]\n",
            "661 [D loss: 1.244961, acc.: 27.34%, op_acc: 94.53%] [G loss: 0.925459]\n",
            "662 [D loss: 1.240446, acc.: 29.30%, op_acc: 93.16%] [G loss: 0.924259]\n",
            "663 [D loss: 0.836642, acc.: 63.87%, op_acc: 94.73%] [G loss: 0.868330]\n",
            "664 [D loss: 0.860637, acc.: 59.18%, op_acc: 95.70%] [G loss: 0.866280]\n",
            "665 [D loss: 0.805741, acc.: 63.48%, op_acc: 93.95%] [G loss: 0.863873]\n",
            "666 [D loss: 0.716719, acc.: 71.29%, op_acc: 95.70%] [G loss: 0.956177]\n",
            "667 [D loss: 0.818382, acc.: 67.97%, op_acc: 94.34%] [G loss: 0.904388]\n",
            "668 [D loss: 0.693386, acc.: 81.05%, op_acc: 94.53%] [G loss: 0.895558]\n",
            "669 [D loss: 0.616610, acc.: 81.25%, op_acc: 94.14%] [G loss: 0.939040]\n",
            "670 [D loss: 0.846525, acc.: 77.34%, op_acc: 90.23%] [G loss: 0.898519]\n",
            "671 [D loss: 0.795387, acc.: 69.14%, op_acc: 94.14%] [G loss: 0.901428]\n",
            "672 [D loss: 0.716768, acc.: 76.56%, op_acc: 94.14%] [G loss: 0.914168]\n",
            "673 [D loss: 0.604458, acc.: 85.16%, op_acc: 96.68%] [G loss: 0.953287]\n",
            "674 [D loss: 0.735227, acc.: 79.88%, op_acc: 93.75%] [G loss: 0.998217]\n",
            "675 [D loss: 0.808204, acc.: 73.63%, op_acc: 92.77%] [G loss: 0.921452]\n",
            "676 [D loss: 0.842989, acc.: 59.77%, op_acc: 93.55%] [G loss: 0.870407]\n",
            "677 [D loss: 0.693168, acc.: 81.05%, op_acc: 92.58%] [G loss: 0.997310]\n",
            "678 [D loss: 0.734676, acc.: 75.78%, op_acc: 94.73%] [G loss: 0.983599]\n",
            "679 [D loss: 0.941310, acc.: 55.66%, op_acc: 94.53%] [G loss: 0.951633]\n",
            "680 [D loss: 0.892418, acc.: 61.13%, op_acc: 93.36%] [G loss: 0.903621]\n",
            "681 [D loss: 1.029076, acc.: 47.66%, op_acc: 92.77%] [G loss: 0.899101]\n",
            "682 [D loss: 0.962185, acc.: 48.24%, op_acc: 94.53%] [G loss: 0.983868]\n",
            "683 [D loss: 1.089140, acc.: 40.62%, op_acc: 93.36%] [G loss: 0.920425]\n",
            "684 [D loss: 0.997663, acc.: 48.83%, op_acc: 93.16%] [G loss: 0.907760]\n",
            "685 [D loss: 1.027851, acc.: 42.19%, op_acc: 93.55%] [G loss: 0.923058]\n",
            "686 [D loss: 0.944293, acc.: 48.83%, op_acc: 93.95%] [G loss: 0.949813]\n",
            "687 [D loss: 1.226265, acc.: 31.84%, op_acc: 93.95%] [G loss: 0.877272]\n",
            "688 [D loss: 1.010882, acc.: 51.17%, op_acc: 93.36%] [G loss: 0.866926]\n",
            "689 [D loss: 0.925822, acc.: 57.62%, op_acc: 95.12%] [G loss: 0.837592]\n",
            "690 [D loss: 0.820624, acc.: 68.95%, op_acc: 93.55%] [G loss: 0.909415]\n",
            "691 [D loss: 0.678088, acc.: 82.62%, op_acc: 93.75%] [G loss: 0.934873]\n",
            "692 [D loss: 0.884553, acc.: 60.35%, op_acc: 93.55%] [G loss: 0.913837]\n",
            "693 [D loss: 0.702005, acc.: 77.15%, op_acc: 95.31%] [G loss: 0.881170]\n",
            "694 [D loss: 0.605607, acc.: 89.26%, op_acc: 94.34%] [G loss: 0.930196]\n",
            "695 [D loss: 0.715276, acc.: 77.73%, op_acc: 93.75%] [G loss: 0.915655]\n",
            "696 [D loss: 0.765314, acc.: 66.80%, op_acc: 94.92%] [G loss: 0.938398]\n",
            "697 [D loss: 0.699214, acc.: 78.71%, op_acc: 93.75%] [G loss: 0.971122]\n",
            "698 [D loss: 0.648786, acc.: 82.42%, op_acc: 94.14%] [G loss: 0.938425]\n",
            "699 [D loss: 0.788653, acc.: 66.60%, op_acc: 93.95%] [G loss: 1.032235]\n",
            "700 [D loss: 0.852569, acc.: 70.12%, op_acc: 92.38%] [G loss: 0.962806]\n",
            "701 [D loss: 0.850722, acc.: 62.30%, op_acc: 93.36%] [G loss: 0.923826]\n",
            "702 [D loss: 1.027026, acc.: 41.60%, op_acc: 95.31%] [G loss: 0.959590]\n",
            "703 [D loss: 0.939463, acc.: 48.83%, op_acc: 94.14%] [G loss: 0.882091]\n",
            "704 [D loss: 1.069193, acc.: 41.02%, op_acc: 92.58%] [G loss: 0.947947]\n",
            "705 [D loss: 0.925523, acc.: 52.34%, op_acc: 94.73%] [G loss: 0.962830]\n",
            "706 [D loss: 1.061389, acc.: 43.75%, op_acc: 93.36%] [G loss: 0.889204]\n",
            "707 [D loss: 0.797719, acc.: 69.92%, op_acc: 93.75%] [G loss: 0.990565]\n",
            "708 [D loss: 0.925032, acc.: 54.88%, op_acc: 93.36%] [G loss: 0.892760]\n",
            "709 [D loss: 0.821070, acc.: 66.99%, op_acc: 94.14%] [G loss: 0.946871]\n",
            "710 [D loss: 0.770910, acc.: 72.07%, op_acc: 93.75%] [G loss: 0.944479]\n",
            "711 [D loss: 0.826278, acc.: 65.82%, op_acc: 94.73%] [G loss: 0.963601]\n",
            "712 [D loss: 0.759402, acc.: 75.00%, op_acc: 92.97%] [G loss: 0.899433]\n",
            "713 [D loss: 0.734755, acc.: 73.05%, op_acc: 94.14%] [G loss: 0.889934]\n",
            "714 [D loss: 0.746776, acc.: 73.05%, op_acc: 94.73%] [G loss: 0.905807]\n",
            "715 [D loss: 0.867090, acc.: 58.59%, op_acc: 94.53%] [G loss: 0.906672]\n",
            "716 [D loss: 0.816069, acc.: 60.35%, op_acc: 95.70%] [G loss: 0.866879]\n",
            "717 [D loss: 0.823475, acc.: 59.18%, op_acc: 93.36%] [G loss: 0.890943]\n",
            "718 [D loss: 1.001433, acc.: 44.92%, op_acc: 94.92%] [G loss: 0.878627]\n",
            "719 [D loss: 0.817879, acc.: 65.43%, op_acc: 93.16%] [G loss: 0.920267]\n",
            "720 [D loss: 0.960786, acc.: 46.88%, op_acc: 94.92%] [G loss: 0.892772]\n",
            "721 [D loss: 0.914567, acc.: 57.23%, op_acc: 92.77%] [G loss: 0.906615]\n",
            "722 [D loss: 0.876709, acc.: 60.35%, op_acc: 94.14%] [G loss: 0.935428]\n",
            "723 [D loss: 0.963611, acc.: 50.59%, op_acc: 94.34%] [G loss: 0.925365]\n",
            "724 [D loss: 0.802899, acc.: 69.53%, op_acc: 92.77%] [G loss: 0.868425]\n",
            "725 [D loss: 0.803457, acc.: 69.53%, op_acc: 94.92%] [G loss: 0.863764]\n",
            "726 [D loss: 0.808602, acc.: 70.70%, op_acc: 92.77%] [G loss: 0.881897]\n",
            "727 [D loss: 0.778498, acc.: 72.27%, op_acc: 93.16%] [G loss: 0.895107]\n",
            "728 [D loss: 0.673023, acc.: 79.30%, op_acc: 94.53%] [G loss: 0.914203]\n",
            "729 [D loss: 0.578857, acc.: 90.04%, op_acc: 95.70%] [G loss: 0.921546]\n",
            "730 [D loss: 0.811624, acc.: 70.31%, op_acc: 91.60%] [G loss: 0.960448]\n",
            "731 [D loss: 0.606145, acc.: 84.38%, op_acc: 94.92%] [G loss: 0.979556]\n",
            "732 [D loss: 0.763614, acc.: 65.82%, op_acc: 96.09%] [G loss: 0.914524]\n",
            "733 [D loss: 0.816135, acc.: 65.04%, op_acc: 93.36%] [G loss: 0.898669]\n",
            "734 [D loss: 0.699096, acc.: 76.95%, op_acc: 93.55%] [G loss: 0.992042]\n",
            "735 [D loss: 1.063164, acc.: 51.56%, op_acc: 89.65%] [G loss: 0.962820]\n",
            "736 [D loss: 1.079075, acc.: 40.82%, op_acc: 93.16%] [G loss: 0.953270]\n",
            "737 [D loss: 1.177723, acc.: 30.86%, op_acc: 94.53%] [G loss: 0.949452]\n",
            "738 [D loss: 1.302999, acc.: 17.97%, op_acc: 95.90%] [G loss: 0.911292]\n",
            "739 [D loss: 1.356877, acc.: 18.95%, op_acc: 94.53%] [G loss: 0.907655]\n",
            "740 [D loss: 1.218082, acc.: 26.37%, op_acc: 93.16%] [G loss: 0.886336]\n",
            "741 [D loss: 1.164701, acc.: 36.33%, op_acc: 93.95%] [G loss: 0.869637]\n",
            "742 [D loss: 0.965674, acc.: 50.59%, op_acc: 92.58%] [G loss: 0.860416]\n",
            "743 [D loss: 0.893937, acc.: 55.66%, op_acc: 94.92%] [G loss: 0.810531]\n",
            "744 [D loss: 0.675264, acc.: 83.01%, op_acc: 92.77%] [G loss: 0.822398]\n",
            "745 [D loss: 0.732521, acc.: 73.05%, op_acc: 94.34%] [G loss: 0.826703]\n",
            "746 [D loss: 0.624396, acc.: 86.13%, op_acc: 93.55%] [G loss: 0.891091]\n",
            "747 [D loss: 0.502761, acc.: 91.99%, op_acc: 95.31%] [G loss: 0.892034]\n",
            "748 [D loss: 0.627656, acc.: 85.94%, op_acc: 93.95%] [G loss: 0.894242]\n",
            "749 [D loss: 0.572696, acc.: 89.65%, op_acc: 94.14%] [G loss: 0.985545]\n",
            "750 [D loss: 0.602949, acc.: 88.48%, op_acc: 94.92%] [G loss: 0.998118]\n",
            "751 [D loss: 0.596409, acc.: 87.70%, op_acc: 93.55%] [G loss: 0.962923]\n",
            "752 [D loss: 0.617229, acc.: 91.60%, op_acc: 90.62%] [G loss: 1.028274]\n",
            "753 [D loss: 0.532771, acc.: 92.19%, op_acc: 93.75%] [G loss: 0.971662]\n",
            "754 [D loss: 0.523439, acc.: 95.31%, op_acc: 92.19%] [G loss: 1.052768]\n",
            "755 [D loss: 0.596797, acc.: 87.50%, op_acc: 94.92%] [G loss: 1.042500]\n",
            "756 [D loss: 0.704269, acc.: 77.93%, op_acc: 94.34%] [G loss: 0.984384]\n",
            "757 [D loss: 0.798706, acc.: 72.46%, op_acc: 92.97%] [G loss: 1.065671]\n",
            "758 [D loss: 0.933349, acc.: 54.30%, op_acc: 94.34%] [G loss: 0.964276]\n",
            "759 [D loss: 1.040969, acc.: 38.48%, op_acc: 94.92%] [G loss: 0.891046]\n",
            "760 [D loss: 1.125957, acc.: 41.21%, op_acc: 94.14%] [G loss: 0.973731]\n",
            "761 [D loss: 1.214551, acc.: 26.95%, op_acc: 93.55%] [G loss: 0.976364]\n",
            "762 [D loss: 1.286234, acc.: 23.83%, op_acc: 93.36%] [G loss: 0.947955]\n",
            "763 [D loss: 1.279662, acc.: 21.68%, op_acc: 94.34%] [G loss: 0.886026]\n",
            "764 [D loss: 1.274698, acc.: 17.77%, op_acc: 95.51%] [G loss: 0.885225]\n",
            "765 [D loss: 1.035799, acc.: 37.70%, op_acc: 94.53%] [G loss: 0.881508]\n",
            "766 [D loss: 0.926661, acc.: 54.88%, op_acc: 94.73%] [G loss: 0.839768]\n",
            "767 [D loss: 0.887978, acc.: 57.42%, op_acc: 94.53%] [G loss: 0.869446]\n",
            "768 [D loss: 0.940057, acc.: 55.86%, op_acc: 93.75%] [G loss: 0.843722]\n",
            "769 [D loss: 0.653716, acc.: 80.27%, op_acc: 95.51%] [G loss: 0.840206]\n",
            "770 [D loss: 0.648363, acc.: 79.69%, op_acc: 95.12%] [G loss: 0.841522]\n",
            "771 [D loss: 0.605743, acc.: 84.18%, op_acc: 94.14%] [G loss: 0.881617]\n",
            "772 [D loss: 0.560738, acc.: 92.97%, op_acc: 93.36%] [G loss: 0.855551]\n",
            "773 [D loss: 0.488131, acc.: 96.09%, op_acc: 93.55%] [G loss: 0.903057]\n",
            "774 [D loss: 0.491319, acc.: 93.75%, op_acc: 94.34%] [G loss: 0.905333]\n",
            "775 [D loss: 0.550625, acc.: 96.29%, op_acc: 91.99%] [G loss: 0.979445]\n",
            "776 [D loss: 0.453073, acc.: 95.51%, op_acc: 95.90%] [G loss: 0.974283]\n",
            "777 [D loss: 0.470377, acc.: 97.46%, op_acc: 92.77%] [G loss: 1.036656]\n",
            "778 [D loss: 0.456583, acc.: 93.75%, op_acc: 95.51%] [G loss: 0.987826]\n",
            "779 [D loss: 0.545282, acc.: 93.16%, op_acc: 93.75%] [G loss: 1.026382]\n",
            "780 [D loss: 0.410108, acc.: 97.66%, op_acc: 94.34%] [G loss: 1.085752]\n",
            "781 [D loss: 0.539883, acc.: 91.99%, op_acc: 94.92%] [G loss: 1.084192]\n",
            "782 [D loss: 0.511475, acc.: 92.77%, op_acc: 93.95%] [G loss: 1.129851]\n",
            "783 [D loss: 0.580658, acc.: 88.48%, op_acc: 93.75%] [G loss: 1.078785]\n",
            "784 [D loss: 0.592245, acc.: 88.09%, op_acc: 93.36%] [G loss: 0.976807]\n",
            "785 [D loss: 0.869493, acc.: 62.70%, op_acc: 92.58%] [G loss: 1.069993]\n",
            "786 [D loss: 0.724849, acc.: 70.90%, op_acc: 95.51%] [G loss: 1.006041]\n",
            "787 [D loss: 0.887326, acc.: 55.66%, op_acc: 95.51%] [G loss: 1.022939]\n",
            "788 [D loss: 1.068994, acc.: 42.97%, op_acc: 93.55%] [G loss: 1.041479]\n",
            "789 [D loss: 1.282783, acc.: 25.78%, op_acc: 93.55%] [G loss: 0.934458]\n",
            "790 [D loss: 1.151094, acc.: 33.59%, op_acc: 94.73%] [G loss: 0.978499]\n",
            "791 [D loss: 1.217529, acc.: 21.88%, op_acc: 95.31%] [G loss: 0.977015]\n",
            "792 [D loss: 1.083657, acc.: 37.89%, op_acc: 93.75%] [G loss: 0.929456]\n",
            "793 [D loss: 1.075812, acc.: 36.33%, op_acc: 93.95%] [G loss: 0.909650]\n",
            "794 [D loss: 0.973890, acc.: 44.92%, op_acc: 94.73%] [G loss: 0.832758]\n",
            "795 [D loss: 0.884509, acc.: 55.86%, op_acc: 95.12%] [G loss: 0.852290]\n",
            "796 [D loss: 0.718759, acc.: 68.55%, op_acc: 96.09%] [G loss: 0.847668]\n",
            "797 [D loss: 0.811978, acc.: 61.72%, op_acc: 94.34%] [G loss: 0.882045]\n",
            "798 [D loss: 0.690218, acc.: 78.91%, op_acc: 95.12%] [G loss: 0.912893]\n",
            "799 [D loss: 0.930183, acc.: 53.12%, op_acc: 93.75%] [G loss: 0.863971]\n",
            "800 [D loss: 0.766813, acc.: 76.76%, op_acc: 92.38%] [G loss: 0.882691]\n",
            "801 [D loss: 0.609652, acc.: 80.86%, op_acc: 95.51%] [G loss: 0.848410]\n",
            "802 [D loss: 0.741298, acc.: 74.22%, op_acc: 94.73%] [G loss: 0.851525]\n",
            "803 [D loss: 0.706506, acc.: 74.61%, op_acc: 94.34%] [G loss: 0.877013]\n",
            "804 [D loss: 0.828051, acc.: 66.80%, op_acc: 93.36%] [G loss: 0.869256]\n",
            "805 [D loss: 0.678753, acc.: 83.40%, op_acc: 93.75%] [G loss: 0.947637]\n",
            "806 [D loss: 0.790301, acc.: 65.43%, op_acc: 95.51%] [G loss: 0.924961]\n",
            "807 [D loss: 0.730434, acc.: 69.14%, op_acc: 95.51%] [G loss: 0.885805]\n",
            "808 [D loss: 1.029628, acc.: 43.55%, op_acc: 93.16%] [G loss: 0.905727]\n",
            "809 [D loss: 0.964685, acc.: 51.17%, op_acc: 94.14%] [G loss: 0.980268]\n",
            "810 [D loss: 0.866205, acc.: 56.64%, op_acc: 95.51%] [G loss: 0.988022]\n",
            "811 [D loss: 1.035559, acc.: 37.70%, op_acc: 96.29%] [G loss: 0.943084]\n",
            "812 [D loss: 0.923433, acc.: 54.30%, op_acc: 93.75%] [G loss: 0.981980]\n",
            "813 [D loss: 1.184389, acc.: 28.52%, op_acc: 94.73%] [G loss: 0.962553]\n",
            "814 [D loss: 0.869554, acc.: 62.11%, op_acc: 92.77%] [G loss: 0.886999]\n",
            "815 [D loss: 1.136555, acc.: 32.81%, op_acc: 95.31%] [G loss: 0.883385]\n",
            "816 [D loss: 1.076631, acc.: 40.04%, op_acc: 93.55%] [G loss: 0.903592]\n",
            "817 [D loss: 1.019559, acc.: 43.95%, op_acc: 94.34%] [G loss: 0.884040]\n",
            "818 [D loss: 1.053107, acc.: 38.48%, op_acc: 93.75%] [G loss: 0.950470]\n",
            "819 [D loss: 1.063444, acc.: 34.96%, op_acc: 95.70%] [G loss: 0.882928]\n",
            "820 [D loss: 0.968222, acc.: 49.80%, op_acc: 93.55%] [G loss: 0.881795]\n",
            "821 [D loss: 1.072224, acc.: 35.16%, op_acc: 95.31%] [G loss: 0.889375]\n",
            "822 [D loss: 0.937496, acc.: 54.10%, op_acc: 94.14%] [G loss: 0.871600]\n",
            "823 [D loss: 0.937008, acc.: 50.00%, op_acc: 95.12%] [G loss: 0.819413]\n",
            "824 [D loss: 1.025937, acc.: 39.84%, op_acc: 94.92%] [G loss: 0.865226]\n",
            "825 [D loss: 0.870279, acc.: 57.03%, op_acc: 94.14%] [G loss: 0.839339]\n",
            "826 [D loss: 0.825725, acc.: 63.48%, op_acc: 94.73%] [G loss: 0.895196]\n",
            "827 [D loss: 0.838917, acc.: 60.35%, op_acc: 94.34%] [G loss: 0.860316]\n",
            "828 [D loss: 0.762498, acc.: 75.00%, op_acc: 93.16%] [G loss: 0.849160]\n",
            "829 [D loss: 0.793232, acc.: 66.99%, op_acc: 93.36%] [G loss: 0.903477]\n",
            "830 [D loss: 0.577312, acc.: 87.11%, op_acc: 94.92%] [G loss: 0.909767]\n",
            "831 [D loss: 0.783147, acc.: 66.80%, op_acc: 94.14%] [G loss: 0.945667]\n",
            "832 [D loss: 0.693326, acc.: 75.78%, op_acc: 94.92%] [G loss: 0.900390]\n",
            "833 [D loss: 0.724872, acc.: 75.20%, op_acc: 94.34%] [G loss: 0.906933]\n",
            "834 [D loss: 0.641658, acc.: 81.25%, op_acc: 95.12%] [G loss: 0.950676]\n",
            "835 [D loss: 0.731702, acc.: 68.75%, op_acc: 95.12%] [G loss: 0.914540]\n",
            "836 [D loss: 0.800212, acc.: 62.50%, op_acc: 95.51%] [G loss: 0.969472]\n",
            "837 [D loss: 0.763100, acc.: 70.51%, op_acc: 94.92%] [G loss: 1.018884]\n",
            "838 [D loss: 0.926045, acc.: 46.88%, op_acc: 96.29%] [G loss: 0.919078]\n",
            "839 [D loss: 1.183955, acc.: 28.71%, op_acc: 93.75%] [G loss: 0.921138]\n",
            "840 [D loss: 1.182749, acc.: 27.15%, op_acc: 95.51%] [G loss: 0.899974]\n",
            "841 [D loss: 1.263915, acc.: 22.27%, op_acc: 95.51%] [G loss: 0.885008]\n",
            "842 [D loss: 1.188210, acc.: 21.88%, op_acc: 97.27%] [G loss: 0.892098]\n",
            "843 [D loss: 1.044281, acc.: 37.89%, op_acc: 94.53%] [G loss: 0.941007]\n",
            "844 [D loss: 1.133104, acc.: 26.95%, op_acc: 96.09%] [G loss: 0.871453]\n",
            "845 [D loss: 1.123007, acc.: 31.64%, op_acc: 95.12%] [G loss: 0.849005]\n",
            "846 [D loss: 1.108254, acc.: 31.25%, op_acc: 93.55%] [G loss: 0.860185]\n",
            "847 [D loss: 0.733380, acc.: 78.32%, op_acc: 92.77%] [G loss: 0.895949]\n",
            "848 [D loss: 0.828942, acc.: 61.33%, op_acc: 94.73%] [G loss: 0.834727]\n",
            "849 [D loss: 0.653951, acc.: 79.30%, op_acc: 95.90%] [G loss: 0.814594]\n",
            "850 [D loss: 0.601685, acc.: 81.64%, op_acc: 95.31%] [G loss: 0.862133]\n",
            "851 [D loss: 0.539387, acc.: 87.30%, op_acc: 95.31%] [G loss: 0.878003]\n",
            "852 [D loss: 0.481535, acc.: 93.95%, op_acc: 94.92%] [G loss: 0.947552]\n",
            "853 [D loss: 0.475430, acc.: 94.53%, op_acc: 93.75%] [G loss: 0.965447]\n",
            "854 [D loss: 0.440272, acc.: 97.27%, op_acc: 94.53%] [G loss: 1.000607]\n",
            "855 [D loss: 0.471570, acc.: 97.46%, op_acc: 92.19%] [G loss: 0.998459]\n",
            "856 [D loss: 0.395606, acc.: 98.44%, op_acc: 95.31%] [G loss: 0.975295]\n",
            "857 [D loss: 0.365877, acc.: 98.83%, op_acc: 94.73%] [G loss: 0.978855]\n",
            "858 [D loss: 0.468714, acc.: 95.70%, op_acc: 92.58%] [G loss: 1.008888]\n",
            "859 [D loss: 0.368379, acc.: 99.80%, op_acc: 93.75%] [G loss: 1.099668]\n",
            "860 [D loss: 0.405169, acc.: 97.66%, op_acc: 94.92%] [G loss: 1.105282]\n",
            "861 [D loss: 0.339528, acc.: 97.85%, op_acc: 96.09%] [G loss: 1.100741]\n",
            "862 [D loss: 0.486869, acc.: 93.75%, op_acc: 94.73%] [G loss: 0.998575]\n",
            "863 [D loss: 0.524402, acc.: 94.92%, op_acc: 94.14%] [G loss: 1.142958]\n",
            "864 [D loss: 0.535009, acc.: 92.97%, op_acc: 94.53%] [G loss: 1.032430]\n",
            "865 [D loss: 0.680312, acc.: 82.81%, op_acc: 92.77%] [G loss: 1.086647]\n",
            "866 [D loss: 0.722276, acc.: 79.88%, op_acc: 91.80%] [G loss: 1.059603]\n",
            "867 [D loss: 0.810163, acc.: 56.84%, op_acc: 94.92%] [G loss: 0.994888]\n",
            "868 [D loss: 0.784555, acc.: 63.28%, op_acc: 94.92%] [G loss: 1.027304]\n",
            "869 [D loss: 1.134389, acc.: 32.23%, op_acc: 95.70%] [G loss: 0.939270]\n",
            "870 [D loss: 1.216358, acc.: 26.37%, op_acc: 95.70%] [G loss: 0.888190]\n",
            "871 [D loss: 1.382944, acc.: 18.36%, op_acc: 94.92%] [G loss: 0.914296]\n",
            "872 [D loss: 1.317029, acc.: 19.92%, op_acc: 95.31%] [G loss: 0.989524]\n",
            "873 [D loss: 1.274760, acc.: 20.70%, op_acc: 95.70%] [G loss: 0.934397]\n",
            "874 [D loss: 0.942299, acc.: 42.77%, op_acc: 96.29%] [G loss: 0.939322]\n",
            "875 [D loss: 0.987785, acc.: 44.14%, op_acc: 94.14%] [G loss: 0.949401]\n",
            "876 [D loss: 1.134498, acc.: 38.87%, op_acc: 93.95%] [G loss: 0.887664]\n",
            "877 [D loss: 0.939141, acc.: 50.78%, op_acc: 95.31%] [G loss: 0.920105]\n",
            "878 [D loss: 0.768324, acc.: 63.67%, op_acc: 95.70%] [G loss: 0.874499]\n",
            "879 [D loss: 0.791532, acc.: 64.65%, op_acc: 95.12%] [G loss: 0.867734]\n",
            "880 [D loss: 0.765722, acc.: 72.85%, op_acc: 93.95%] [G loss: 0.871055]\n",
            "881 [D loss: 0.582418, acc.: 87.50%, op_acc: 94.53%] [G loss: 0.899060]\n",
            "882 [D loss: 0.529090, acc.: 88.67%, op_acc: 95.31%] [G loss: 0.830764]\n",
            "883 [D loss: 0.557671, acc.: 91.60%, op_acc: 93.95%] [G loss: 0.871244]\n",
            "884 [D loss: 0.446375, acc.: 96.29%, op_acc: 94.14%] [G loss: 0.919745]\n",
            "885 [D loss: 0.544965, acc.: 94.34%, op_acc: 92.19%] [G loss: 0.884109]\n",
            "886 [D loss: 0.399801, acc.: 98.24%, op_acc: 94.53%] [G loss: 0.893485]\n",
            "887 [D loss: 0.347026, acc.: 98.05%, op_acc: 95.51%] [G loss: 0.949376]\n",
            "888 [D loss: 0.392574, acc.: 99.02%, op_acc: 94.14%] [G loss: 1.002543]\n",
            "889 [D loss: 0.423047, acc.: 99.22%, op_acc: 93.55%] [G loss: 1.049019]\n",
            "890 [D loss: 0.401459, acc.: 96.48%, op_acc: 96.09%] [G loss: 1.016965]\n",
            "891 [D loss: 0.401566, acc.: 98.83%, op_acc: 93.16%] [G loss: 0.984291]\n",
            "892 [D loss: 0.351706, acc.: 99.22%, op_acc: 95.12%] [G loss: 1.043797]\n",
            "893 [D loss: 0.422728, acc.: 96.68%, op_acc: 95.31%] [G loss: 1.033588]\n",
            "894 [D loss: 0.525276, acc.: 93.36%, op_acc: 93.55%] [G loss: 1.031593]\n",
            "895 [D loss: 0.414008, acc.: 97.66%, op_acc: 93.55%] [G loss: 1.067289]\n",
            "896 [D loss: 0.502315, acc.: 92.19%, op_acc: 94.53%] [G loss: 1.020914]\n",
            "897 [D loss: 0.507792, acc.: 88.67%, op_acc: 96.09%] [G loss: 1.012883]\n",
            "898 [D loss: 0.845309, acc.: 62.50%, op_acc: 93.95%] [G loss: 1.039254]\n",
            "899 [D loss: 0.799660, acc.: 61.72%, op_acc: 96.88%] [G loss: 0.943279]\n",
            "900 [D loss: 1.007412, acc.: 46.48%, op_acc: 93.95%] [G loss: 0.959394]\n",
            "901 [D loss: 1.219358, acc.: 29.69%, op_acc: 95.31%] [G loss: 0.923153]\n",
            "902 [D loss: 1.407083, acc.: 17.77%, op_acc: 94.53%] [G loss: 0.937457]\n",
            "903 [D loss: 1.447558, acc.: 15.43%, op_acc: 93.36%] [G loss: 0.887581]\n",
            "904 [D loss: 1.216030, acc.: 25.20%, op_acc: 94.53%] [G loss: 0.903582]\n",
            "905 [D loss: 1.211203, acc.: 24.41%, op_acc: 96.09%] [G loss: 0.926375]\n",
            "906 [D loss: 1.305220, acc.: 17.19%, op_acc: 96.09%] [G loss: 0.906819]\n",
            "907 [D loss: 1.117244, acc.: 31.45%, op_acc: 93.55%] [G loss: 0.874385]\n",
            "908 [D loss: 1.244864, acc.: 23.24%, op_acc: 95.31%] [G loss: 0.882123]\n",
            "909 [D loss: 1.004672, acc.: 42.77%, op_acc: 94.92%] [G loss: 0.881616]\n",
            "910 [D loss: 1.073097, acc.: 34.96%, op_acc: 93.95%] [G loss: 0.790862]\n",
            "911 [D loss: 0.874876, acc.: 56.64%, op_acc: 96.29%] [G loss: 0.856555]\n",
            "912 [D loss: 0.847061, acc.: 58.79%, op_acc: 95.12%] [G loss: 0.868935]\n",
            "913 [D loss: 0.905281, acc.: 54.49%, op_acc: 94.73%] [G loss: 0.815185]\n",
            "914 [D loss: 0.731020, acc.: 78.91%, op_acc: 93.75%] [G loss: 0.816146]\n",
            "915 [D loss: 0.663730, acc.: 80.08%, op_acc: 96.09%] [G loss: 0.827507]\n",
            "916 [D loss: 0.565424, acc.: 85.74%, op_acc: 95.31%] [G loss: 0.848499]\n",
            "917 [D loss: 0.638754, acc.: 80.47%, op_acc: 93.36%] [G loss: 0.851819]\n",
            "918 [D loss: 0.444717, acc.: 97.66%, op_acc: 94.53%] [G loss: 0.955292]\n",
            "919 [D loss: 0.428724, acc.: 95.31%, op_acc: 95.51%] [G loss: 0.897358]\n",
            "920 [D loss: 0.383818, acc.: 99.41%, op_acc: 93.95%] [G loss: 0.918498]\n",
            "921 [D loss: 0.396902, acc.: 98.05%, op_acc: 94.34%] [G loss: 1.015876]\n",
            "922 [D loss: 0.369667, acc.: 98.44%, op_acc: 95.90%] [G loss: 0.995269]\n",
            "923 [D loss: 0.382719, acc.: 98.05%, op_acc: 95.51%] [G loss: 0.963594]\n",
            "924 [D loss: 0.430136, acc.: 98.05%, op_acc: 93.95%] [G loss: 1.057534]\n",
            "925 [D loss: 0.327354, acc.: 100.00%, op_acc: 94.73%] [G loss: 1.072338]\n",
            "926 [D loss: 0.330174, acc.: 98.24%, op_acc: 95.31%] [G loss: 1.145342]\n",
            "927 [D loss: 0.400325, acc.: 98.05%, op_acc: 94.73%] [G loss: 1.071522]\n",
            "928 [D loss: 0.452379, acc.: 95.90%, op_acc: 95.51%] [G loss: 1.039626]\n",
            "929 [D loss: 0.427554, acc.: 96.09%, op_acc: 95.51%] [G loss: 1.010022]\n",
            "930 [D loss: 0.425245, acc.: 98.24%, op_acc: 93.36%] [G loss: 1.107386]\n",
            "931 [D loss: 0.511785, acc.: 90.82%, op_acc: 93.36%] [G loss: 0.970663]\n",
            "932 [D loss: 0.506462, acc.: 91.80%, op_acc: 94.34%] [G loss: 1.087726]\n",
            "933 [D loss: 0.626537, acc.: 83.40%, op_acc: 94.34%] [G loss: 1.006641]\n",
            "934 [D loss: 0.718239, acc.: 70.31%, op_acc: 95.51%] [G loss: 0.997171]\n",
            "935 [D loss: 0.583613, acc.: 85.35%, op_acc: 95.90%] [G loss: 1.025894]\n",
            "936 [D loss: 0.930728, acc.: 51.37%, op_acc: 96.29%] [G loss: 0.973365]\n",
            "937 [D loss: 1.198223, acc.: 31.84%, op_acc: 94.14%] [G loss: 0.851281]\n",
            "938 [D loss: 1.013715, acc.: 42.97%, op_acc: 95.51%] [G loss: 0.908826]\n",
            "939 [D loss: 1.293023, acc.: 24.22%, op_acc: 93.55%] [G loss: 0.902056]\n",
            "940 [D loss: 1.103924, acc.: 32.62%, op_acc: 95.90%] [G loss: 0.894169]\n",
            "941 [D loss: 0.987713, acc.: 44.92%, op_acc: 94.14%] [G loss: 0.867307]\n",
            "942 [D loss: 1.061450, acc.: 36.52%, op_acc: 95.70%] [G loss: 0.963581]\n",
            "943 [D loss: 1.153429, acc.: 30.08%, op_acc: 93.36%] [G loss: 0.898123]\n",
            "944 [D loss: 1.048066, acc.: 41.60%, op_acc: 94.92%] [G loss: 0.891711]\n",
            "945 [D loss: 1.015326, acc.: 40.23%, op_acc: 94.53%] [G loss: 0.859861]\n",
            "946 [D loss: 0.929204, acc.: 49.41%, op_acc: 94.34%] [G loss: 0.872203]\n",
            "947 [D loss: 0.691666, acc.: 74.02%, op_acc: 94.53%] [G loss: 0.931311]\n",
            "948 [D loss: 0.707298, acc.: 72.07%, op_acc: 95.90%] [G loss: 0.818213]\n",
            "949 [D loss: 0.765488, acc.: 68.95%, op_acc: 93.95%] [G loss: 0.846515]\n",
            "950 [D loss: 0.901353, acc.: 56.45%, op_acc: 95.70%] [G loss: 0.824827]\n",
            "951 [D loss: 0.641014, acc.: 79.69%, op_acc: 95.51%] [G loss: 0.861444]\n",
            "952 [D loss: 0.711773, acc.: 73.05%, op_acc: 95.51%] [G loss: 0.893987]\n",
            "953 [D loss: 0.636104, acc.: 83.98%, op_acc: 93.75%] [G loss: 0.918701]\n",
            "954 [D loss: 0.601548, acc.: 86.72%, op_acc: 95.31%] [G loss: 0.947565]\n",
            "955 [D loss: 0.694081, acc.: 77.73%, op_acc: 95.51%] [G loss: 0.900825]\n",
            "956 [D loss: 0.666898, acc.: 76.76%, op_acc: 95.90%] [G loss: 0.856951]\n",
            "957 [D loss: 0.638612, acc.: 81.64%, op_acc: 96.09%] [G loss: 0.979289]\n",
            "958 [D loss: 0.691836, acc.: 81.45%, op_acc: 93.75%] [G loss: 0.972157]\n",
            "959 [D loss: 0.817711, acc.: 62.30%, op_acc: 94.14%] [G loss: 0.981344]\n",
            "960 [D loss: 0.684576, acc.: 75.59%, op_acc: 95.90%] [G loss: 0.985822]\n",
            "961 [D loss: 1.070637, acc.: 38.48%, op_acc: 93.75%] [G loss: 0.956416]\n",
            "962 [D loss: 0.939487, acc.: 48.44%, op_acc: 93.75%] [G loss: 0.994155]\n",
            "963 [D loss: 1.277691, acc.: 22.85%, op_acc: 93.95%] [G loss: 0.896283]\n",
            "964 [D loss: 1.400528, acc.: 13.28%, op_acc: 94.92%] [G loss: 0.936938]\n",
            "965 [D loss: 1.589597, acc.: 7.23%, op_acc: 94.14%] [G loss: 0.902304]\n",
            "966 [D loss: 1.550128, acc.: 8.01%, op_acc: 94.53%] [G loss: 0.873831]\n",
            "967 [D loss: 1.241864, acc.: 23.24%, op_acc: 94.92%] [G loss: 0.867347]\n",
            "968 [D loss: 1.047239, acc.: 37.70%, op_acc: 94.53%] [G loss: 0.833642]\n",
            "969 [D loss: 1.031916, acc.: 40.04%, op_acc: 94.53%] [G loss: 0.830783]\n",
            "970 [D loss: 0.886651, acc.: 57.62%, op_acc: 93.75%] [G loss: 0.829864]\n",
            "971 [D loss: 0.891203, acc.: 57.81%, op_acc: 95.51%] [G loss: 0.800814]\n",
            "972 [D loss: 0.754192, acc.: 69.73%, op_acc: 94.73%] [G loss: 0.806475]\n",
            "973 [D loss: 0.595599, acc.: 84.18%, op_acc: 95.51%] [G loss: 0.816510]\n",
            "974 [D loss: 0.731083, acc.: 74.02%, op_acc: 93.36%] [G loss: 0.806539]\n",
            "975 [D loss: 0.535654, acc.: 93.75%, op_acc: 94.73%] [G loss: 0.817039]\n",
            "976 [D loss: 0.456149, acc.: 94.53%, op_acc: 93.75%] [G loss: 0.869111]\n",
            "977 [D loss: 0.416781, acc.: 95.31%, op_acc: 96.48%] [G loss: 0.885825]\n",
            "978 [D loss: 0.501753, acc.: 96.48%, op_acc: 93.75%] [G loss: 0.965394]\n",
            "979 [D loss: 0.419840, acc.: 99.22%, op_acc: 94.34%] [G loss: 0.917436]\n",
            "980 [D loss: 0.465102, acc.: 95.90%, op_acc: 93.95%] [G loss: 1.028523]\n",
            "981 [D loss: 0.371231, acc.: 99.22%, op_acc: 93.36%] [G loss: 1.069310]\n",
            "982 [D loss: 0.308602, acc.: 99.02%, op_acc: 95.12%] [G loss: 1.043358]\n",
            "983 [D loss: 0.313814, acc.: 98.83%, op_acc: 95.31%] [G loss: 1.032613]\n",
            "984 [D loss: 0.341213, acc.: 100.00%, op_acc: 93.95%] [G loss: 1.033562]\n",
            "985 [D loss: 0.274944, acc.: 99.80%, op_acc: 96.48%] [G loss: 1.097255]\n",
            "986 [D loss: 0.293253, acc.: 100.00%, op_acc: 95.12%] [G loss: 1.080093]\n",
            "987 [D loss: 0.327064, acc.: 99.41%, op_acc: 93.95%] [G loss: 1.151594]\n",
            "988 [D loss: 0.347768, acc.: 99.41%, op_acc: 94.92%] [G loss: 1.099538]\n",
            "989 [D loss: 0.398064, acc.: 97.46%, op_acc: 94.73%] [G loss: 1.107863]\n",
            "990 [D loss: 0.402544, acc.: 97.46%, op_acc: 93.36%] [G loss: 1.147604]\n",
            "991 [D loss: 0.497953, acc.: 96.88%, op_acc: 93.36%] [G loss: 1.077223]\n",
            "992 [D loss: 0.503845, acc.: 95.70%, op_acc: 93.36%] [G loss: 1.033067]\n",
            "993 [D loss: 0.568470, acc.: 83.79%, op_acc: 95.51%] [G loss: 0.976215]\n",
            "994 [D loss: 0.556570, acc.: 88.48%, op_acc: 95.31%] [G loss: 1.052393]\n",
            "995 [D loss: 0.633474, acc.: 78.32%, op_acc: 95.12%] [G loss: 1.086011]\n",
            "996 [D loss: 0.999094, acc.: 41.80%, op_acc: 96.09%] [G loss: 1.055552]\n",
            "997 [D loss: 0.986848, acc.: 49.02%, op_acc: 94.34%] [G loss: 0.963350]\n",
            "998 [D loss: 0.936515, acc.: 50.20%, op_acc: 95.51%] [G loss: 0.890955]\n",
            "999 [D loss: 1.360790, acc.: 15.62%, op_acc: 95.70%] [G loss: 0.944465]\n",
            "1000 [D loss: 1.234398, acc.: 29.49%, op_acc: 94.34%] [G loss: 0.850544]\n",
            "1001 [D loss: 1.321156, acc.: 20.51%, op_acc: 94.73%] [G loss: 0.874997]\n",
            "1002 [D loss: 1.134571, acc.: 28.91%, op_acc: 95.70%] [G loss: 0.840627]\n",
            "1003 [D loss: 1.431220, acc.: 11.33%, op_acc: 94.92%] [G loss: 0.853414]\n",
            "1004 [D loss: 1.263801, acc.: 26.17%, op_acc: 94.73%] [G loss: 0.852569]\n",
            "1005 [D loss: 1.168761, acc.: 31.05%, op_acc: 94.92%] [G loss: 0.917485]\n",
            "1006 [D loss: 1.047944, acc.: 39.26%, op_acc: 94.73%] [G loss: 0.887604]\n",
            "1007 [D loss: 1.037168, acc.: 35.16%, op_acc: 95.12%] [G loss: 0.840007]\n",
            "1008 [D loss: 1.038596, acc.: 44.34%, op_acc: 95.12%] [G loss: 0.859488]\n",
            "1009 [D loss: 0.886311, acc.: 57.23%, op_acc: 93.16%] [G loss: 0.835595]\n",
            "1010 [D loss: 0.845780, acc.: 63.28%, op_acc: 93.55%] [G loss: 0.883226]\n",
            "1011 [D loss: 0.992380, acc.: 42.77%, op_acc: 93.55%] [G loss: 0.774292]\n",
            "1012 [D loss: 0.812178, acc.: 69.73%, op_acc: 92.77%] [G loss: 0.827063]\n",
            "1013 [D loss: 0.705027, acc.: 81.64%, op_acc: 91.99%] [G loss: 0.882462]\n",
            "1014 [D loss: 0.606334, acc.: 83.40%, op_acc: 94.34%] [G loss: 0.832589]\n",
            "1015 [D loss: 0.589898, acc.: 85.35%, op_acc: 93.75%] [G loss: 0.805246]\n",
            "1016 [D loss: 0.498888, acc.: 94.92%, op_acc: 95.70%] [G loss: 0.925241]\n",
            "1017 [D loss: 0.558406, acc.: 91.02%, op_acc: 94.73%] [G loss: 0.921436]\n",
            "1018 [D loss: 0.508838, acc.: 95.12%, op_acc: 93.36%] [G loss: 0.933244]\n",
            "1019 [D loss: 0.503040, acc.: 93.75%, op_acc: 94.34%] [G loss: 0.932681]\n",
            "1020 [D loss: 0.529688, acc.: 94.14%, op_acc: 94.14%] [G loss: 0.953262]\n",
            "1021 [D loss: 0.561932, acc.: 89.84%, op_acc: 95.12%] [G loss: 0.920775]\n",
            "1022 [D loss: 0.450215, acc.: 95.51%, op_acc: 94.92%] [G loss: 0.973107]\n",
            "1023 [D loss: 0.452424, acc.: 95.90%, op_acc: 94.34%] [G loss: 0.973913]\n",
            "1024 [D loss: 0.448454, acc.: 96.29%, op_acc: 95.31%] [G loss: 0.987946]\n",
            "1025 [D loss: 0.532386, acc.: 92.19%, op_acc: 93.75%] [G loss: 0.884399]\n",
            "1026 [D loss: 0.605438, acc.: 82.62%, op_acc: 94.14%] [G loss: 0.966508]\n",
            "1027 [D loss: 0.723430, acc.: 77.54%, op_acc: 94.14%] [G loss: 0.922372]\n",
            "1028 [D loss: 0.830990, acc.: 64.06%, op_acc: 92.97%] [G loss: 0.933927]\n",
            "1029 [D loss: 0.714869, acc.: 69.73%, op_acc: 96.68%] [G loss: 0.962010]\n",
            "1030 [D loss: 0.893100, acc.: 51.17%, op_acc: 96.29%] [G loss: 0.913825]\n",
            "1031 [D loss: 1.030418, acc.: 40.62%, op_acc: 94.92%] [G loss: 0.882953]\n",
            "1032 [D loss: 1.055265, acc.: 45.70%, op_acc: 93.36%] [G loss: 0.936620]\n",
            "1033 [D loss: 1.462650, acc.: 15.62%, op_acc: 92.38%] [G loss: 0.891669]\n",
            "1034 [D loss: 1.488601, acc.: 9.77%, op_acc: 95.12%] [G loss: 0.866599]\n",
            "1035 [D loss: 1.532594, acc.: 14.84%, op_acc: 95.12%] [G loss: 0.848419]\n",
            "1036 [D loss: 1.133703, acc.: 26.37%, op_acc: 96.29%] [G loss: 0.843389]\n",
            "1037 [D loss: 1.025752, acc.: 33.59%, op_acc: 95.70%] [G loss: 0.898493]\n",
            "1038 [D loss: 1.268760, acc.: 28.91%, op_acc: 95.12%] [G loss: 0.853475]\n",
            "1039 [D loss: 0.996538, acc.: 37.70%, op_acc: 97.07%] [G loss: 0.767682]\n",
            "1040 [D loss: 0.801498, acc.: 59.38%, op_acc: 96.29%] [G loss: 0.768529]\n",
            "1041 [D loss: 1.070434, acc.: 34.57%, op_acc: 94.14%] [G loss: 0.835346]\n",
            "1042 [D loss: 0.990802, acc.: 44.92%, op_acc: 95.12%] [G loss: 0.828946]\n",
            "1043 [D loss: 0.830372, acc.: 63.28%, op_acc: 95.70%] [G loss: 0.779522]\n",
            "1044 [D loss: 0.790069, acc.: 65.43%, op_acc: 95.12%] [G loss: 0.836713]\n",
            "1045 [D loss: 0.668248, acc.: 76.37%, op_acc: 96.09%] [G loss: 0.880374]\n",
            "1046 [D loss: 0.615987, acc.: 80.08%, op_acc: 96.88%] [G loss: 0.863670]\n",
            "1047 [D loss: 0.697117, acc.: 80.86%, op_acc: 93.55%] [G loss: 0.894085]\n",
            "1048 [D loss: 0.555688, acc.: 86.72%, op_acc: 94.92%] [G loss: 0.943117]\n",
            "1049 [D loss: 0.643023, acc.: 80.47%, op_acc: 94.92%] [G loss: 0.857739]\n",
            "1050 [D loss: 0.502622, acc.: 94.92%, op_acc: 95.31%] [G loss: 0.895142]\n",
            "1051 [D loss: 0.438393, acc.: 95.12%, op_acc: 95.31%] [G loss: 0.925346]\n",
            "1052 [D loss: 0.435351, acc.: 96.48%, op_acc: 94.53%] [G loss: 0.922644]\n",
            "1053 [D loss: 0.394408, acc.: 98.63%, op_acc: 93.95%] [G loss: 0.937652]\n",
            "1054 [D loss: 0.365767, acc.: 97.27%, op_acc: 95.31%] [G loss: 0.998980]\n",
            "1055 [D loss: 0.403844, acc.: 98.24%, op_acc: 93.16%] [G loss: 0.986517]\n",
            "1056 [D loss: 0.486832, acc.: 96.88%, op_acc: 93.55%] [G loss: 0.945325]\n"
          ]
        }
      ]
    }
  ]
}